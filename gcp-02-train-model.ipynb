{"cells":[{"cell_type":"markdown","source":["# BERT NLP on Verbal Autopsy Dataset for Social Determinants of Health\n","\n","---\n","\n"],"metadata":{"id":"ithHUQQTSAMv"},"id":"ithHUQQTSAMv"},{"cell_type":"markdown","source":["#### Connecting to Google Accounts\n","Authenticating Google account and copying the dataset .csv file to the Colab working directory:"],"metadata":{"id":"ghTeDZAOUFS_"},"id":"ghTeDZAOUFS_"},{"cell_type":"code","source":["from google.colab import auth\n","auth.authenticate_user()\n","\n","!mkdir -p data/processed\n","!gsutil cp gs://kind-lab-data/verbal-autopsy-sdoh/merged_dataset.csv ./data/processed/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ExBGoHlgZob7","executionInfo":{"status":"ok","timestamp":1668192137376,"user_tz":300,"elapsed":14889,"user":{"displayName":"Justin Xu","userId":"13320263107479278707"}},"outputId":"abe40586-7b9c-4000-bee8-a00ea64b1531"},"id":"ExBGoHlgZob7","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Copying gs://kind-lab-data/verbal-autopsy-sdoh/merged_dataset.csv...\n","/ [1 files][  4.1 MiB/  4.1 MiB]                                                \n","Operation completed over 1 objects/4.1 MiB.                                      \n"]}]},{"cell_type":"markdown","source":["### Setup"],"metadata":{"id":"WwJ9Z_UU3mfo"},"id":"WwJ9Z_UU3mfo"},{"cell_type":"markdown","source":["#### Installing & Importing Dependencies"],"metadata":{"id":"E0ft5NBLUVoZ"},"id":"E0ft5NBLUVoZ"},{"cell_type":"code","source":["!pip install transformers evaluate transformers_interpret"],"metadata":{"id":"v_CoTeowZyiB","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1668192152404,"user_tz":300,"elapsed":15041,"user":{"displayName":"Justin Xu","userId":"13320263107479278707"}},"outputId":"0da88ab1-f972-49a7-ce2b-74d3823d22ab"},"id":"v_CoTeowZyiB","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n","\u001b[K     |████████████████████████████████| 5.5 MB 9.0 MB/s \n","\u001b[?25hCollecting evaluate\n","  Downloading evaluate-0.3.0-py3-none-any.whl (72 kB)\n","\u001b[K     |████████████████████████████████| 72 kB 1.6 MB/s \n","\u001b[?25hCollecting transformers_interpret\n","  Downloading transformers_interpret-0.9.6-py3-none-any.whl (45 kB)\n","\u001b[K     |████████████████████████████████| 45 kB 4.4 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 52.8 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n","\u001b[K     |████████████████████████████████| 163 kB 73.4 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Collecting datasets>=2.0.0\n","  Downloading datasets-2.6.1-py3-none-any.whl (441 kB)\n","\u001b[K     |████████████████████████████████| 441 kB 69.3 MB/s \n","\u001b[?25hCollecting multiprocess\n","  Downloading multiprocess-0.70.14-py37-none-any.whl (115 kB)\n","\u001b[K     |████████████████████████████████| 115 kB 72.7 MB/s \n","\u001b[?25hCollecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from evaluate) (1.3.5)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from evaluate) (0.3.6)\n","Collecting xxhash\n","  Downloading xxhash-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 60.8 MB/s \n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from evaluate) (2022.10.0)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets>=2.0.0->evaluate) (3.8.3)\n","Collecting dill\n","  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n","\u001b[K     |████████████████████████████████| 95 kB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.13.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.1.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.1.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 73.3 MB/s \n","\u001b[?25hCollecting captum>=0.3.1\n","  Downloading captum-0.5.0-py3-none-any.whl (1.4 MB)\n","\u001b[K     |████████████████████████████████| 1.4 MB 54.8 MB/s \n","\u001b[?25hCollecting ipython<8.0.0,>=7.31.1\n","  Downloading ipython-7.34.0-py3-none-any.whl (793 kB)\n","\u001b[K     |████████████████████████████████| 793 kB 67.3 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from captum>=0.3.1->transformers_interpret) (1.12.1+cu113)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from captum>=0.3.1->transformers_interpret) (3.2.2)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (4.4.2)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (2.6.1)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (5.1.1)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (2.0.10)\n","Collecting matplotlib-inline\n","  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (57.4.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (0.7.5)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (0.2.0)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (4.8.0)\n","Collecting jedi>=0.16\n","  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 56.6 MB/s \n","\u001b[?25hRequirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython<8.0.0,>=7.31.1->transformers_interpret) (0.8.3)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython<8.0.0,>=7.31.1->transformers_interpret) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8.0.0,>=7.31.1->transformers_interpret) (0.2.5)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8.0.0,>=7.31.1->transformers_interpret) (1.15.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum>=0.3.1->transformers_interpret) (1.4.4)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum>=0.3.1->transformers_interpret) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum>=0.3.1->transformers_interpret) (0.11.0)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n","\u001b[K     |████████████████████████████████| 115 kB 75.6 MB/s \n","\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->evaluate) (2022.6)\n","Installing collected packages: urllib3, dill, xxhash, tokenizers, responses, multiprocess, matplotlib-inline, jedi, huggingface-hub, transformers, ipython, datasets, captum, transformers-interpret, evaluate\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","  Attempting uninstall: dill\n","    Found existing installation: dill 0.3.6\n","    Uninstalling dill-0.3.6:\n","      Successfully uninstalled dill-0.3.6\n","  Attempting uninstall: ipython\n","    Found existing installation: ipython 7.9.0\n","    Uninstalling ipython-7.9.0:\n","      Successfully uninstalled ipython-7.9.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires ipython~=7.9.0, but you have ipython 7.34.0 which is incompatible.\u001b[0m\n","Successfully installed captum-0.5.0 datasets-2.6.1 dill-0.3.5.1 evaluate-0.3.0 huggingface-hub-0.10.1 ipython-7.34.0 jedi-0.18.1 matplotlib-inline-0.1.6 multiprocess-0.70.13 responses-0.18.0 tokenizers-0.13.2 transformers-4.24.0 transformers-interpret-0.9.6 urllib3-1.25.11 xxhash-3.1.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["IPython","urllib3"]}}},"metadata":{}}]},{"cell_type":"code","source":["import time\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","# from numpy.random import default_rng\n","# rng = default_rng(87)\n","\n","import evaluate\n","from transformers import AutoTokenizer\n","from transformers import BertForSequenceClassification, Trainer, TrainingArguments, RobertaForSequenceClassification\n","\n","import torch"],"metadata":{"id":"3VVdUU2NZw7r","executionInfo":{"status":"ok","timestamp":1668192160571,"user_tz":300,"elapsed":8173,"user":{"displayName":"Justin Xu","userId":"13320263107479278707"}}},"id":"3VVdUU2NZw7r","execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["#### Dataset Exploration"],"metadata":{"id":"XhVTemI8VZuG"},"id":"XhVTemI8VZuG"},{"cell_type":"code","execution_count":4,"id":"434ce5de-4755-420d-bd3b-71e2a2df485f","metadata":{"id":"434ce5de-4755-420d-bd3b-71e2a2df485f","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1668192160762,"user_tz":300,"elapsed":202,"user":{"displayName":"Justin Xu","userId":"13320263107479278707"}},"outputId":"aa0d9ba3-91d1-48cd-ce11-1477915872e8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["      newid    site     sex  age_years  age_months  age_days  \\\n","0         1  Mexico    Male       51.0         NaN       NaN   \n","1         2      AP    Male       24.0         NaN       NaN   \n","2         3      AP  Female       62.0         NaN       NaN   \n","3         4  Mexico  Female       80.0         NaN       NaN   \n","4         5      UP    Male       76.0         NaN       NaN   \n","...     ...     ...     ...        ...         ...       ...   \n","6805   7842  Mexico  Female       52.0         NaN       NaN   \n","6806   7843     Dar  Female       78.0         NaN       NaN   \n","6807   7844      AP  Female       25.0         NaN       NaN   \n","6808   7845      UP  Female       22.0         NaN       NaN   \n","6809   7846     Dar    Male       90.0         NaN       NaN   \n","\n","                          gs_text34  \\\n","0                         Cirrhosis   \n","1                          Epilepsy   \n","2                         Pneumonia   \n","3                              COPD   \n","4       Acute Myocardial Infarction   \n","...                             ...   \n","6805                Cervical Cancer   \n","6806  Other Cardiovascular Diseases   \n","6807                     Poisonings   \n","6808                          Fires   \n","6809              Esophageal Cancer   \n","\n","                                          open_response  g1_01d      g1_01m  \\\n","0     no comments.[PERSON] only told us what happene...    17.0      August   \n","1     [PATIENT] was suffering for the last two years...     NaN         NaN   \n","2     she has stopped consuming tablets for b.p and ...     NaN         NaN   \n","3     my mother's condition was already very poor du...    23.0        June   \n","4     the deceased suffered a heart attack 2 days ag...     2.0     October   \n","...                                                 ...     ...         ...   \n","6805  my aunt had been diabetic for the past 9 year....     6.0        July   \n","6806  the participant had nothing to comment on_x000...    -1.0  Don't Know   \n","6807  she opened the tailoring shop in the center be...     NaN         NaN   \n","6808  the deceased had been suffering from pain in t...    16.0         May   \n","6809                 the client thanked for the service    -1.0  Don't Know   \n","\n","      ...  g5_01y   g5_02  g5_03d     g5_03m  g5_03y  g5_04a  g5_04b  g5_04c  \\\n","0     ...  1958.0    Male     NaN  September  2009.0    51.0     NaN     NaN   \n","1     ...     NaN    Male    19.0   November  2008.0    24.0     NaN     NaN   \n","2     ...     NaN  Female     7.0    October  2008.0    62.0     NaN     NaN   \n","3     ...  1923.0  Female    19.0    January  2009.0    80.0     NaN     NaN   \n","4     ...  1933.0    Male    21.0      April  2009.0    76.0     NaN     NaN   \n","...   ...     ...     ...     ...        ...     ...     ...     ...     ...   \n","6805  ...  1957.0  Female    22.0  September  2009.0    52.0     NaN     NaN   \n","6806  ...  1930.0  Female     3.0   November  2008.0    78.0     NaN     NaN   \n","6807  ...  1984.0  Female     2.0   February  2009.0    25.0     NaN     NaN   \n","6808  ...  1987.0  Female    16.0     August  2009.0    22.0     NaN     NaN   \n","6809  ...  1918.0    Male    27.0   February  2008.0    90.0     NaN     NaN   \n","\n","              g5_06a      g5_06b  \n","0     Primary School  Don't Know  \n","1       No Schooling           0  \n","2       No Schooling           0  \n","3       No Schooling           0  \n","4        High School          12  \n","...              ...         ...  \n","6805    No Schooling           0  \n","6806  Primary School           0  \n","6807     High School           0  \n","6808    No Schooling           0  \n","6809  Primary School           0  \n","\n","[6810 rows x 53 columns]"],"text/html":["\n","  <div id=\"df-90aa1234-185f-484d-9d7b-259f21a45a8e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>newid</th>\n","      <th>site</th>\n","      <th>sex</th>\n","      <th>age_years</th>\n","      <th>age_months</th>\n","      <th>age_days</th>\n","      <th>gs_text34</th>\n","      <th>open_response</th>\n","      <th>g1_01d</th>\n","      <th>g1_01m</th>\n","      <th>...</th>\n","      <th>g5_01y</th>\n","      <th>g5_02</th>\n","      <th>g5_03d</th>\n","      <th>g5_03m</th>\n","      <th>g5_03y</th>\n","      <th>g5_04a</th>\n","      <th>g5_04b</th>\n","      <th>g5_04c</th>\n","      <th>g5_06a</th>\n","      <th>g5_06b</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Mexico</td>\n","      <td>Male</td>\n","      <td>51.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Cirrhosis</td>\n","      <td>no comments.[PERSON] only told us what happene...</td>\n","      <td>17.0</td>\n","      <td>August</td>\n","      <td>...</td>\n","      <td>1958.0</td>\n","      <td>Male</td>\n","      <td>NaN</td>\n","      <td>September</td>\n","      <td>2009.0</td>\n","      <td>51.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Primary School</td>\n","      <td>Don't Know</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>AP</td>\n","      <td>Male</td>\n","      <td>24.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Epilepsy</td>\n","      <td>[PATIENT] was suffering for the last two years...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>Male</td>\n","      <td>19.0</td>\n","      <td>November</td>\n","      <td>2008.0</td>\n","      <td>24.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>No Schooling</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>AP</td>\n","      <td>Female</td>\n","      <td>62.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Pneumonia</td>\n","      <td>she has stopped consuming tablets for b.p and ...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>Female</td>\n","      <td>7.0</td>\n","      <td>October</td>\n","      <td>2008.0</td>\n","      <td>62.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>No Schooling</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>Mexico</td>\n","      <td>Female</td>\n","      <td>80.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>COPD</td>\n","      <td>my mother's condition was already very poor du...</td>\n","      <td>23.0</td>\n","      <td>June</td>\n","      <td>...</td>\n","      <td>1923.0</td>\n","      <td>Female</td>\n","      <td>19.0</td>\n","      <td>January</td>\n","      <td>2009.0</td>\n","      <td>80.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>No Schooling</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>UP</td>\n","      <td>Male</td>\n","      <td>76.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Acute Myocardial Infarction</td>\n","      <td>the deceased suffered a heart attack 2 days ag...</td>\n","      <td>2.0</td>\n","      <td>October</td>\n","      <td>...</td>\n","      <td>1933.0</td>\n","      <td>Male</td>\n","      <td>21.0</td>\n","      <td>April</td>\n","      <td>2009.0</td>\n","      <td>76.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>High School</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>6805</th>\n","      <td>7842</td>\n","      <td>Mexico</td>\n","      <td>Female</td>\n","      <td>52.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Cervical Cancer</td>\n","      <td>my aunt had been diabetic for the past 9 year....</td>\n","      <td>6.0</td>\n","      <td>July</td>\n","      <td>...</td>\n","      <td>1957.0</td>\n","      <td>Female</td>\n","      <td>22.0</td>\n","      <td>September</td>\n","      <td>2009.0</td>\n","      <td>52.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>No Schooling</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6806</th>\n","      <td>7843</td>\n","      <td>Dar</td>\n","      <td>Female</td>\n","      <td>78.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Other Cardiovascular Diseases</td>\n","      <td>the participant had nothing to comment on_x000...</td>\n","      <td>-1.0</td>\n","      <td>Don't Know</td>\n","      <td>...</td>\n","      <td>1930.0</td>\n","      <td>Female</td>\n","      <td>3.0</td>\n","      <td>November</td>\n","      <td>2008.0</td>\n","      <td>78.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Primary School</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6807</th>\n","      <td>7844</td>\n","      <td>AP</td>\n","      <td>Female</td>\n","      <td>25.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Poisonings</td>\n","      <td>she opened the tailoring shop in the center be...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>1984.0</td>\n","      <td>Female</td>\n","      <td>2.0</td>\n","      <td>February</td>\n","      <td>2009.0</td>\n","      <td>25.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>High School</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6808</th>\n","      <td>7845</td>\n","      <td>UP</td>\n","      <td>Female</td>\n","      <td>22.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Fires</td>\n","      <td>the deceased had been suffering from pain in t...</td>\n","      <td>16.0</td>\n","      <td>May</td>\n","      <td>...</td>\n","      <td>1987.0</td>\n","      <td>Female</td>\n","      <td>16.0</td>\n","      <td>August</td>\n","      <td>2009.0</td>\n","      <td>22.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>No Schooling</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6809</th>\n","      <td>7846</td>\n","      <td>Dar</td>\n","      <td>Male</td>\n","      <td>90.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Esophageal Cancer</td>\n","      <td>the client thanked for the service</td>\n","      <td>-1.0</td>\n","      <td>Don't Know</td>\n","      <td>...</td>\n","      <td>1918.0</td>\n","      <td>Male</td>\n","      <td>27.0</td>\n","      <td>February</td>\n","      <td>2008.0</td>\n","      <td>90.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Primary School</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>6810 rows × 53 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-90aa1234-185f-484d-9d7b-259f21a45a8e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-90aa1234-185f-484d-9d7b-259f21a45a8e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-90aa1234-185f-484d-9d7b-259f21a45a8e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}],"source":["df = pd.read_csv('data/processed/merged_dataset.csv')\n","df"]},{"cell_type":"markdown","source":["Determining lengths of records in the dataset using RoBERTa:"],"metadata":{"id":"mxrzDaq-XdNm"},"id":"mxrzDaq-XdNm"},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","sentences = df['open_response'].tolist()\n","encoded = tokenizer(sentences, truncation=False)\n","\n","seq_lengths = [np.count_nonzero(x) for x in encoded['input_ids']]\n","idx_long_seq = [i for i, x in enumerate(seq_lengths) if x > 512]\n","print(f'{len(idx_long_seq)} sentences longer than 512 tokens.')\n","print(f'Lengths: {sorted([seq_lengths[i] for i in idx_long_seq])}')\n","# for i in idx_long_seq[:1]:\n","#      print(tokenizer.decode(encoded['input_ids'][i]))\n"],"metadata":{"id":"KXsegi9I11Kt","colab":{"base_uri":"https://localhost:8080/","height":0,"referenced_widgets":["a31db033354245bcba0937764108c813","5645c4cc81b4411eb8312d05fa74888a","57899bddd571446fbd4279815d651021","81bf2c4f15b2455bbb80e5cac80d774e","1fd84fe1b5ee40b6b18f0aa12e601c03","06105feeee1c481cab396ab1a73d9276","97360fd5c3854398a9ae8109525e932d","b5ad953ba313413296aa1fdb5c9a5b8b","96a2fce3ffca4326a12ffbc068ae5a7a","b95b81eb19c44ce98cf2a29c7d7c8373","a397cf9f2a5c4ecbbd9d84b4559fb84b","38d6ed2bf6c44d81893f32ff0a448b38","2f62e19f754540bea612fd8bea0d8531","4683ced2246d4c16ae567430d054afff","41c7dc16eba8477ebd149f2e60cf6d3d","f9594fa587b34a7f831929dce288b036","b3841c7b4fca432d94b574c51493ddcf","04d81f735d06437183969995c7ecbf64","e4e96a99b8b74cb6800a6dd136d68bad","72ca6294e99a43c4a43b3289302c4367","8797c015403f4c5d9db61cecf5aa68be","2a1e72a9c6a94e4885d7fb17f59f42fe","b46abdd7772544848c5a49f4ef67188b","757af5f6557c496696fdab4b1a5cca09","5f109f7463564ac9ba2be62d51550f32","8aed795ce59f4cfeafb719ecf723e695","c563997b2e2645759ee6cc6fa79df413","84adf48dc3624a03815fa0073ee871b9","f8b382622a084f39b37aa0e040306b04","ef0a0170ab884b37a1d860e99c8843ec","096b41d856844cffa6d2500588e9b059","35777e7d0cc34d31ad781ead8ca67c4e","898845e7c69f4c74bc6e95a6904212d4","67e7166b71ae4c8dafd986ef36c87470","b90eb48fc1c4400da43e24d105f0e505","017dbb114995445abe466165f1b4ddfa","6e068e4846c7458ead18db9a983d5730","99eece6541854e16a2a696c293509713","aba950e16f704dac9fc1d20249236aae","cae3b6d947fa40458df269c01468b675","097431f0085f4204bdc49c843f7df820","c41a0f0455a447368d3e2059a190c4b5","2f70419e6f4647f186d209f6fb2e8f3b","20157452c28a4056ac3fed3c8694826d"]},"executionInfo":{"status":"ok","timestamp":1668192167534,"user_tz":300,"elapsed":6775,"user":{"displayName":"Justin Xu","userId":"13320263107479278707"}},"outputId":"08f14cbb-ebec-4c72-ff10-4b8550fc6f57"},"id":"KXsegi9I11Kt","execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a31db033354245bcba0937764108c813"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38d6ed2bf6c44d81893f32ff0a448b38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b46abdd7772544848c5a49f4ef67188b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67e7166b71ae4c8dafd986ef36c87470"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (533 > 512). Running this sequence through the model will result in indexing errors\n"]},{"output_type":"stream","name":"stdout","text":["27 sentences longer than 512 tokens.\n","Lengths: [513, 518, 519, 520, 521, 521, 526, 529, 532, 533, 535, 543, 543, 544, 545, 545, 551, 559, 559, 573, 583, 592, 597, 604, 622, 627, 803]\n"]}]},{"cell_type":"markdown","source":["And using BERT:"],"metadata":{"id":"sk4NS5wT57hS"},"id":"sk4NS5wT57hS"},{"cell_type":"code","execution_count":6,"id":"c38297e4-d720-4d9e-aa54-f9caf3a39ff0","metadata":{"id":"c38297e4-d720-4d9e-aa54-f9caf3a39ff0","colab":{"base_uri":"https://localhost:8080/","height":0,"referenced_widgets":["427082df5e8d45778b9597efdc7deb6e","dfe09275fe9248ebb3ad7f783b97c456","24ad4c401953458ea278d18140f75c49","10c686f5101f48489e4ef0d6c86adf5f","a5ee06de7a4b464aaa9ade8646c6c6f2","0971f51261c447ec8a8ec67a1afe6639","64e29477e69241e59eb6dac18ffb5a30","00528b45220b41dc89416a5097fc24e2","905ba61e0c0e4a428b1b30ab953ff10c","79713d149d654e5b8cc17684068e5dad","771c0d286607441897228d181c91aab4","ce7878827bdc4ac48435ff447cf5e881","428223f5f04c452ca718377515cfa730","6ac2abed8d75412f88731058cbda76ba","578d5f679f89461a8c007623464548f9","dcccea5ec24346fda61c1bc6714937fa","623be4e7ec884d669f0b8e304d4d40a0","ce631303afd64aee890e947810ddbe4a","c27b1bc00ec848edbcc30548146d13b3","d076abb487694a3ab2127bbefd20b992","5d88883a0a954593b4dbacfb8081ea2a","7c212c9765344da5b1c63838dc17e2d3","d16153d4a95140ea8d14cd4026cb6eea","0b355387f8c0460cac760120460ab39d","f7a5dbcaa48c4ee8afe19e2714817315","b474a4a52a4640e589f59afd87aa7b39","f3ec5561d8da4781bb177b6d2c7891e0","d6dbb317bf9b4102b715785424d661a2","ee069f65cc7246eabc3f60e366dd3410","b3a96b921f994bc5bad74fe151090987","515bd1fae02d488da83f032bb3c13b2d","f41a60d40806490fac9d3627a73a9f97","1eb7d539fd204a669d4d5a28ac2adc7e","2763ac83339f48dfb3e6cead080845da","f1f6618249cc43a3b19b1fb8b8754a48","5f6d82b7e87c40cbb9c649d6b23f6f6e","c47fa99539ed449686f1aa0aede3e2bd","8ad95787097e406e939899dc4b0e0a05","1ee4306bb67a4a138665aaf29f0c9434","24ffc49127ee42bcab9f757db8ba6565","661682f02b4d4105a9e16d110dcb832e","7c135910dad144e18044d26582a52f89","2334e5de866343aba7f2ac0d0b8de5df","84b76e1d52bb40f0b2fea289d03e5ab4"]},"executionInfo":{"status":"ok","timestamp":1668192173975,"user_tz":300,"elapsed":6450,"user":{"displayName":"Justin Xu","userId":"13320263107479278707"}},"outputId":"525adde0-5189-4bb7-9828-9e26c06bfcaf"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"427082df5e8d45778b9597efdc7deb6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce7878827bdc4ac48435ff447cf5e881"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d16153d4a95140ea8d14cd4026cb6eea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2763ac83339f48dfb3e6cead080845da"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (531 > 512). Running this sequence through the model will result in indexing errors\n"]},{"output_type":"stream","name":"stdout","text":["20 sentences longer than 512 tokens.\n","Lengths: [513, 521, 528, 531, 532, 537, 539, 541, 545, 556, 558, 559, 561, 569, 572, 576, 580, 597, 614, 953]\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","sentences = df['open_response'].tolist()\n","encoded = tokenizer(sentences, truncation=False)\n","\n","seq_lengths = [np.count_nonzero(x) for x in encoded['input_ids']]\n","idx_long_seq = [i for i, x in enumerate(seq_lengths) if x > 512]\n","print(f'{len(idx_long_seq)} sentences longer than 512 tokens.')\n","print(f'Lengths: {sorted([seq_lengths[i] for i in idx_long_seq])}')\n","# for i in idx_long_seq[:1]:\n","#      print(tokenizer.decode(encoded['input_ids'][i]))\n"]},{"cell_type":"markdown","id":"c49c3b4e-56cb-4dbd-ac6b-640c670a5550","metadata":{"id":"c49c3b4e-56cb-4dbd-ac6b-640c670a5550"},"source":["There are a few records that have sequence lengths longer than 512. Given the small number of records, we decide to truncate directly rather than splitting the sentences in some way.\n"," "]},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","ax = sns.countplot(df.g5_06a)\n","for p in ax.patches:\n","   ax.annotate('{:1d}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()))\n","plt.xlabel('EducationLevel');"],"metadata":{"id":"Dzf1xZy9yzw6","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1668192174354,"user_tz":300,"elapsed":390,"user":{"displayName":"Justin Xu","userId":"13320263107479278707"}},"outputId":"ffba5098-51cb-4b18-e353-80ae87951b39"},"id":"Dzf1xZy9yzw6","execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n","  FutureWarning\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["df['g5_06a'].value_counts().plot(kind='pie')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"oRweA6Je40_-","executionInfo":{"status":"ok","timestamp":1668192174355,"user_tz":300,"elapsed":7,"user":{"displayName":"Justin Xu","userId":"13320263107479278707"}},"outputId":"c4d23885-92ec-473c-f776-7450bf8c9c4e"},"id":"oRweA6Je40_-","execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7fcf7e1dfa50>"]},"metadata":{},"execution_count":8},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"eGFEr49e5sHH"},"id":"eGFEr49e5sHH"},{"cell_type":"markdown","source":["### Predicting Education Level - 4 Classes"],"metadata":{"id":"ruDQnsrR1qRO"},"id":"ruDQnsrR1qRO"},{"cell_type":"markdown","id":"1e25713a-c8ae-4a3e-94e3-1957970c36f7","metadata":{"id":"1e25713a-c8ae-4a3e-94e3-1957970c36f7"},"source":["#### HuggingFace Dataset\n","Encoding labels:"]},{"cell_type":"code","execution_count":9,"id":"edacef32-804a-4930-b443-dc932eab6f8e","metadata":{"id":"edacef32-804a-4930-b443-dc932eab6f8e","executionInfo":{"status":"ok","timestamp":1668192174577,"user_tz":300,"elapsed":227,"user":{"displayName":"Justin Xu","userId":"13320263107479278707"}}},"outputs":[],"source":["documents = df['open_response'].tolist()\n","label2id = {\n","    'No Schooling': 0,\n","    'Primary School': 1,\n","    'High School': 2,\n","    'College or Higher': 3\n","}\n","labels = df['g5_06a'].map(label2id).tolist()"]},{"cell_type":"markdown","source":["Splitting data into Train/Validation/Test according to 68%/17%/15%:"],"metadata":{"id":"rG2t8u8MuqNR"},"id":"rG2t8u8MuqNR"},{"cell_type":"code","source":["# create train/test split\n","train_texts, test_texts, train_labels, test_labels = train_test_split(documents, labels, test_size=.15, random_state=8573)\n","\n","# create train/validation split\n","train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2, random_state=3820)"],"metadata":{"id":"OIoaMRNaupMZ"},"id":"OIoaMRNaupMZ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Encoding documents using BERT tokenizer:"],"metadata":{"id":"3wJ-5rgZ0I2s"},"id":"3wJ-5rgZ0I2s"},{"cell_type":"code","source":["# tokenize \n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n","val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n","test_encodings = tokenizer(test_texts, truncation=True, padding=True)"],"metadata":{"id":"ALSnYoal0IVS"},"id":"ALSnYoal0IVS","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Defining dataset object compatible with HuggingFace: "],"metadata":{"id":"GsKEND2O0S6b"},"id":"GsKEND2O0S6b"},{"cell_type":"code","source":["# create a class for the dataset -> compatible with huggingface trainer\n","class VerbalAutopsyDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = VerbalAutopsyDataset(train_encodings, train_labels)\n","val_dataset = VerbalAutopsyDataset(val_encodings, val_labels)\n","test_dataset = VerbalAutopsyDataset(test_encodings, test_labels)"],"metadata":{"id":"_W-gbFFm0TWJ"},"id":"_W-gbFFm0TWJ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Model Tuning\n","Fine-tuning BERT model:"],"metadata":{"id":"BX3BVM_o0s2q"},"id":"BX3BVM_o0s2q"},{"cell_type":"code","source":["start_time = time.time()\n","training_args = TrainingArguments(\n","    output_dir='./results',          # output directory\n","    num_train_epochs=10,              # total number of training epochs\n","    per_device_train_batch_size=16,  # batch size per device during training\n","    per_device_eval_batch_size=64,   # batch size for evaluation\n","    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n","    weight_decay=0.01,               # strength of weight decay\n","    logging_dir='./logs',            # directory for storing logs\n","    logging_steps=100,\n","    evaluation_strategy='steps',\n","    eval_steps=100,\n",")\n","\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n","\n","trainer = Trainer(\n","    model=model,                         # the instantiated 🤗 Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=val_dataset             # evaluation dataset\n",")\n","\n","trainer.train()\n","print(f\"Time Elapsed: {time.time() - start_time}\")"],"metadata":{"id":"-ZNM_xn96BLc"},"id":"-ZNM_xn96BLc","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Freezing all but final classifier layer in BERT:"],"metadata":{"id":"3fCkcxyN6DPA"},"id":"3fCkcxyN6DPA"},{"cell_type":"code","execution_count":null,"id":"dc540192-9124-4261-9567-defbfc2c4ae5","metadata":{"id":"dc540192-9124-4261-9567-defbfc2c4ae5","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["400b17fecd5946afb64e94cb38af23fa","6a50631f89cd47f7bf966bfd440caa83","155efd5cc36d4c51943618c7fd8442b9","c6b26a52f59047bea3b89d8a4c5a4001","65c14868d1594576a261b78323f67026","8ec6a3e87e2243529a5c9ca5b5342a8d","2627ffea34f0414cac27fc4a90d60546","d961bb1c79024fc39be7c1bfead655d3","743e5c3bdfcb42f0add66dcd126d13b8","622ca94b471345aeb383a31c2036c743","aa3c9a9e05a44fd497be86af6e799d62"]},"outputId":"e986c3e7-80d9-4a5a-fc6d-a8a7dccc6502","executionInfo":{"status":"error","timestamp":1667889047530,"user_tz":300,"elapsed":657345,"user":{"displayName":"Justin Xu","userId":"13320263107479278707"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"400b17fecd5946afb64e94cb38af23fa"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 4630\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2900\n","  Number of trainable parameters = 3076\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='709' max='2900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 709/2900 10:28 < 32:26, 1.13 it/s, Epoch 2.44/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>1.289800</td>\n","      <td>1.286581</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>1.282600</td>\n","      <td>1.274488</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>1.264100</td>\n","      <td>1.267493</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>1.251600</td>\n","      <td>1.261712</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>1.265100</td>\n","      <td>1.259783</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>1.257600</td>\n","      <td>1.260774</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>1.246700</td>\n","      <td>1.263678</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1158\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1158\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1158\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1158\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1158\n","  Batch size = 64\n","Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 1158\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1158\n","  Batch size = 64\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-f929032312b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Time Elapsed: {time.time() - start_time}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1503\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         )\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1752\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m                     \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_tpu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1754\u001b[0;31m                     \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1755\u001b[0m                 ):\n\u001b[1;32m   1756\u001b[0m                     \u001b[0;31m# if loss is nan or inf simply add the average of previous logged losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["start_time = time.time()\n","training_args = TrainingArguments(\n","    output_dir='./results',          # output directory\n","    num_train_epochs=10,              # total number of training epochs\n","    per_device_train_batch_size=16,  # batch size per device during training\n","    per_device_eval_batch_size=64,   # batch size for evaluation\n","    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n","    weight_decay=0.01,               # strength of weight decay\n","    logging_dir='./logs',            # directory for storing logs\n","    logging_steps=100,\n","    evaluation_strategy='steps',\n","    eval_steps=100,\n",")\n","\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n","\n","# freezing all but final classifier layer\n","for name, param in model.named_parameters():\n","\tif 'classifier' not in name: # classifier layer\n","\t\tparam.requires_grad = False\n","\n","trainer = Trainer(\n","    model=model,                         # the instantiated 🤗 Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=val_dataset             # evaluation dataset\n",")\n","\n","trainer.train()\n","print(f\"Time Elapsed: {time.time() - start_time}\")"]},{"cell_type":"markdown","source":["#### F1 Performance Score"],"metadata":{"id":"0YTSAFmL2est"},"id":"0YTSAFmL2est"},{"cell_type":"code","source":["predictions = trainer.predict(test_dataset)\n","preds = np.argmax(predictions.predictions, axis=-1)\n","f1_metric = evaluate.load(\"f1\")\n","results = f1_metric.compute(predictions=preds, references=predictions.label_ids, average='weighted')\n","print(results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86},"id":"RimM1xFem5si","executionInfo":{"status":"ok","timestamp":1666971642301,"user_tz":240,"elapsed":36378,"user":{"displayName":"Justin Xu","userId":"13320263107479278707"}},"outputId":"cae93509-70b7-4e1b-91ea-5a9be5a982a5"},"id":"RimM1xFem5si","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running Prediction *****\n","  Num examples = 1022\n","  Batch size = 64\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["(1022, 4) (1022,)\n"]}]},{"cell_type":"markdown","source":["#### Plot Training/Validation Curves"],"metadata":{"id":"-Qa8j4XDjmtI"},"id":"-Qa8j4XDjmtI"},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","history = pd.read_excel('Temp.xlsx')\n","plt.plot(history['Step'], history['Training Loss'], label='Training Loss')\n","plt.plot(history['Step'], history['Validation Loss'], label='Validation Loss')\n","\n","plt.title('Loss history')\n","plt.ylabel('Loss')\n","plt.xlabel('Step')\n","plt.legend()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312},"id":"VYQHwvvkukzS","executionInfo":{"status":"ok","timestamp":1666929365688,"user_tz":240,"elapsed":345,"user":{"displayName":"Justin Xu","userId":"13320263107479278707"}},"outputId":"8c8003f2-2ec7-414f-acd6-93b6168ad4e3"},"id":"VYQHwvvkukzS","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.legend.Legend at 0x7feed5e96b90>"]},"metadata":{},"execution_count":31},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["#### Visualize Explanations\n"],"metadata":{"id":"mHu0-qXCqz-c"},"id":"mHu0-qXCqz-c"},{"cell_type":"markdown","source":["Picking the highest predicted value for each class:"],"metadata":{"id":"8dTzOcdNrVIS"},"id":"8dTzOcdNrVIS"},{"cell_type":"code","source":["id2label = {v: k for k, v in label2id.items()}\n","print(id2label)\n","\n","from transformers_interpret import SequenceClassificationExplainer\n","multiclass_explainer = SequenceClassificationExplainer(model=model, tokenizer=tokenizer)\n","max_predictions = predictions.predictions.max(0)\n","print(max_predictions)\n","\n","examples = {}\n","for each_class in id2label.keys():\n","    examples[each_class] = {}\n","    examples[each_class]['max'] = int(np.where(predictions.predictions[:, each_class] == max_predictions[each_class])[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6K_NTsMCrzCv","executionInfo":{"status":"ok","timestamp":1665769865836,"user_tz":240,"elapsed":15,"user":{"displayName":"Justin Xu","userId":"13320263107479278707"}},"outputId":"9388d018-7f6f-4045-feda-d73224413d1a"},"id":"6K_NTsMCrzCv","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'No Schooling': 0,\n"," 'Primary School': 1,\n"," 'High School': 2,\n"," 'College or Higher': 3}"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["Visualizing explanations for the max sample in each class:"],"metadata":{"id":"nry0uLje7KXG"},"id":"nry0uLje7KXG"},{"cell_type":"code","source":["model.to('cpu')\n","for each_class in examples.keys():\n","    for each_example in examples[each_class].keys():\n","        text_example = test_texts[examples[each_class][each_example]]\n","        word_attributions = multiclass_explainer(text=text_example)\n","        print(f\"Prediction for the {each_example} predicted value example of class {each_class}: {multiclass_explainer.predicted_class_name}\")\n","        html = multiclass_explainer.visualize()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":599},"id":"wY5xBN3PzsFt","executionInfo":{"status":"error","timestamp":1665770012524,"user_tz":240,"elapsed":2565,"user":{"displayName":"Justin Xu","userId":"13320263107479278707"}},"outputId":"96985ada-df84-4cff-bcbc-adea8cd24479"},"id":"wY5xBN3PzsFt","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction for the max predicted value example of class 0: LABEL_0\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_0 (0.82)</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_0</b></text></td><td><text style=\"padding-right:2em\"><b>3.38</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> my                    </font></mark><mark style=\"background-color: hsl(120, 75%, 72%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> mother                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> did                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> not                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> have                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> any                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> kind                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sickness                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> she                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> fell                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> down                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> house                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> we                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> took                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> her                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> immediately                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> hospital                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> place                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> doctor                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> said                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> her                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> b                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> p                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> high                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> told                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> us                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> take                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> her                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> hospital                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##2                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> so                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> immediately                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> we                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> took                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> her                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> hospital                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##2                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> while                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> she                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> receiving                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> treatment                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> there                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> she                    </font></mark><mark style=\"background-color: hsl(120, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> died                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"]},"metadata":{}},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-7f49f276abca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0meach_example\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meach_class\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mtext_example\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meach_class\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meach_example\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mword_attributions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulticlass_explainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_example\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Prediction for the {each_example} predicted value example of class {each_class}: {multiclass_explainer.predicted_class_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulticlass_explainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers_interpret/explainers/text/sequence_classification.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, index, class_name, embedding_type, internal_batch_size, n_steps)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minternal_batch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minternal_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers_interpret/explainers/text/sequence_classification.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, text, index, class_name, embedding_type)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_attributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_attributions\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers_interpret/explainers/text/sequence_classification.py\u001b[0m in \u001b[0;36m_calculate_attributions\u001b[0;34m(self, embeddings, index, class_name)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mref_token_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref_token_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0minternal_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         )\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers_interpret/attributions.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, custom_forward, embeddings, tokens, input_ids, ref_input_ids, sep_id, attention_mask, target, token_type_ids, position_ids, ref_token_type_ids, ref_position_ids, internal_batch_size, n_steps)\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0madditional_forward_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0minternal_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             )\n\u001b[1;32m     64\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/captum/log/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/captum/attr/_core/layer/layer_integrated_gradients.py\u001b[0m in \u001b[0;36mattribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta, attribute_to_layer_input)\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0minternal_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minternal_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             \u001b[0mreturn_convergence_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m         )\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/captum/attr/_core/integrated_gradients.py\u001b[0m in \u001b[0;36mattribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta)\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0madditional_forward_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madditional_forward_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m             )\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/captum/attr/_core/integrated_gradients.py\u001b[0m in \u001b[0;36m_attribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, step_sizes_and_alphas)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaled_features_tpl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mtarget_ind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0madditional_forward_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_additional_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m         )\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/captum/attr/_core/layer/layer_integrated_gradients.py\u001b[0m in \u001b[0;36mgradient_func\u001b[0;34m(forward_fn, inputs, target_ind, additional_forward_args)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m                     output = _run_forward(\n\u001b[0;32m--> 465\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_forward_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m                     )\n\u001b[1;32m    467\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/captum/_utils/common.py\u001b[0m in \u001b[0;36m_run_forward\u001b[0;34m(forward_func, inputs, target, additional_forward_args)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0madditional_forward_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madditional_forward_args\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m         \u001b[0;32melse\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m     )\n\u001b[1;32m    461\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_select_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers_interpret/explainers/text/sequence_classification.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, attention_mask)\u001b[0m\n\u001b[1;32m    175\u001b[0m     ):\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers_interpret/explainer.py\u001b[0m in \u001b[0;36m_get_preds\u001b[0;34m(self, input_ids, token_type_ids, position_ids, attention_mask)\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m             )\n\u001b[1;32m    194\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1559\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1560\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1561\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1562\u001b[0m         )\n\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1022\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m         )\n\u001b[1;32m   1026\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    608\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m                 )\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         )\n\u001b[1;32m    496\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         )\n\u001b[1;32m    428\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relative_key\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relative_key_query\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 14.76 GiB total capacity; 13.20 GiB already allocated; 61.75 MiB free; 13.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"U1E9S9mq6XoZ"},"id":"U1E9S9mq6XoZ"},{"cell_type":"markdown","source":["### Predicting Education Level - 2 Classes (Binarized)\n"],"metadata":{"id":"Ulrngldt7UPQ"},"id":"Ulrngldt7UPQ"},{"cell_type":"markdown","metadata":{"id":"0wWxMm-37UPQ"},"source":["#### HuggingFace Dataset\n","Encoding labels:"],"id":"0wWxMm-37UPQ"},{"cell_type":"code","execution_count":null,"metadata":{"id":"LeZxyDOk7UPQ"},"outputs":[],"source":["documents = df['open_response'].tolist()\n","label2id = {\n","    'No Schooling': 0,\n","    'Primary School': 0,\n","    'High School': 1,\n","    'College or Higher': 1\n","}\n","labels = df['g5_06a'].map(label2id).tolist()"],"id":"LeZxyDOk7UPQ"},{"cell_type":"markdown","source":["Splitting data into Train/Validation/Test according to 68%/17%/15%:"],"metadata":{"id":"GDU5OcjG7UPR"},"id":"GDU5OcjG7UPR"},{"cell_type":"code","source":["# create train/test split\n","train_texts, test_texts, train_labels, test_labels = train_test_split(documents, labels, test_size=.15, random_state=8573)\n","\n","# create train/validation split\n","train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2, random_state=3820)"],"metadata":{"id":"BUV5tiAr7UPR"},"execution_count":null,"outputs":[],"id":"BUV5tiAr7UPR"},{"cell_type":"markdown","source":["Encoding documents using RoBERTa tokenizer:"],"metadata":{"id":"TPIQ7_DX7UPR"},"id":"TPIQ7_DX7UPR"},{"cell_type":"code","source":["# tokenize \n","tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n","val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n","test_encodings = tokenizer(test_texts, truncation=True, padding=True)"],"metadata":{"id":"ES-miW-F53Ko"},"id":"ES-miW-F53Ko","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Or using BERT tokenizer:"],"metadata":{"id":"pNS1kxeWEGPG"},"id":"pNS1kxeWEGPG"},{"cell_type":"code","source":["# tokenize \n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n","val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n","test_encodings = tokenizer(test_texts, truncation=True, padding=True)"],"metadata":{"id":"J7PSPVQo7UPR"},"execution_count":null,"outputs":[],"id":"J7PSPVQo7UPR"},{"cell_type":"markdown","source":["Defining dataset object compatible with HuggingFace: "],"metadata":{"id":"ZtKW_j9y7UPR"},"id":"ZtKW_j9y7UPR"},{"cell_type":"code","source":["# create a class for the dataset -> compatible with huggingface trainer\n","class VerbalAutopsyDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = VerbalAutopsyDataset(train_encodings, train_labels)\n","val_dataset = VerbalAutopsyDataset(val_encodings, val_labels)\n","test_dataset = VerbalAutopsyDataset(test_encodings, test_labels)"],"metadata":{"id":"Y4KSN9cV7UPR"},"execution_count":null,"outputs":[],"id":"Y4KSN9cV7UPR"},{"cell_type":"markdown","source":["#### Model Tuning\n","Fine-tuning BERT model:"],"metadata":{"id":"3mjaxHKW7UPR"},"id":"3mjaxHKW7UPR"},{"cell_type":"code","source":["start_time = time.time()\n","training_args = TrainingArguments(\n","    output_dir='./results',          # output directory\n","    num_train_epochs=10,              # total number of training epochs\n","    per_device_train_batch_size=16,  # batch size per device during training\n","    per_device_eval_batch_size=64,   # batch size for evaluation\n","    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n","    weight_decay=0.001,               # strength of weight decay\n","    logging_dir='./logs',            # directory for storing logs\n","    logging_steps=100,\n","    evaluation_strategy='steps',\n","    eval_steps=100,\n",")\n","\n","model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n","\n","trainer = Trainer(\n","    model=model,                         # the instantiated 🤗 Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=val_dataset             # evaluation dataset\n",")\n","\n","trainer.train()\n","print(f\"Time Elapsed: {time.time() - start_time}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["9b02249a8d6744108e53ddaf24a080b2","d6f8d79db754427d901dfe8eebc1d2b8","9c3fff5a29d34c8fa1a9d54932c82653","c8add35dab854c69984082a867006498","7a0e2b87915a46948e8ca794ee108ca0","c3114195fbce46ae94de3de657946dc0","78cd482726c64f6da2587cb32927d3cb","3488165062c243a2bc7865d5600dba93","fdac075851b040b2a3bdd3e59ee0204a","721d5f3cfbc044c2a7a356bc8279d814","3541d8c13d74489c8629668ef2ecd77a"]},"id":"OXxW3L-f7UPR","outputId":"efe11200-3b52-4d41-87cd-6c03343b4c9a","executionInfo":{"status":"error","timestamp":1667940409275,"user_tz":300,"elapsed":1151747,"user":{"displayName":"Ice Gates","userId":"11701491213595641525"}}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b02249a8d6744108e53ddaf24a080b2"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 4630\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2900\n","  Number of trainable parameters = 124647170\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='623' max='2900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 623/2900 18:43 < 1:08:41, 0.55 it/s, Epoch 2.14/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>0.636800</td>\n","      <td>0.608847</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.651800</td>\n","      <td>0.605526</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.622000</td>\n","      <td>0.600146</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.609000</td>\n","      <td>0.602899</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.651100</td>\n","      <td>0.694626</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.631600</td>\n","      <td>0.624061</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1158\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1158\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1158\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1158\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1158\n","  Batch size = 64\n","Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 1158\n","  Batch size = 64\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-cd0b2d7fc3b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Time Elapsed: {time.time() - start_time}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1503\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         )\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1747\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m                 if (\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2524\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2525\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2526\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2528\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"id":"OXxW3L-f7UPR"},{"cell_type":"markdown","source":["Freezing all but final classifier layer in BERT:"],"metadata":{"id":"51-1Hr0t7UPR"},"id":"51-1Hr0t7UPR"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"05d0476e-6f72-4c4d-b808-97e5e64e4a8d","id":"QK3DHbaq7UPR","executionInfo":{"status":"ok","timestamp":1667946084031,"user_tz":300,"elapsed":1968524,"user":{"displayName":"Ice Gates","userId":"11701491213595641525"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/config.json\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.24.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/pytorch_model.bin\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 4630\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 730\n","  Number of trainable parameters = 1538\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='730' max='730' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [730/730 32:41, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>0.671900</td>\n","      <td>0.651727</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.644300</td>\n","      <td>0.629126</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.627100</td>\n","      <td>0.615452</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.621200</td>\n","      <td>0.609356</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.622100</td>\n","      <td>0.607892</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.620500</td>\n","      <td>0.607612</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.616900</td>\n","      <td>0.607456</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1158\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1158\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1158\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1158\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1158\n","  Batch size = 64\n","Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 1158\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1158\n","  Batch size = 64\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Time Elapsed: 1968.357655286789\n"]}],"source":["start_time = time.time()\n","training_args = TrainingArguments(\n","    output_dir='./results',          # output directory\n","    num_train_epochs=10,              # total number of training epochs\n","    per_device_train_batch_size=64,  # batch size per device during training\n","    per_device_eval_batch_size=64,   # batch size for evaluation\n","    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n","    weight_decay=0.0001,               # strength of weight decay\n","    logging_dir='./logs',            # directory for storing logs\n","    logging_steps=100,\n","    evaluation_strategy='steps',\n","    eval_steps=100,\n",")\n","\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n","\n","# freezing all but final classifier layer\n","for name, param in model.named_parameters():\n","\tif 'classifier' not in name: # classifier layer\n","\t\tparam.requires_grad = False\n","\n","# for name, param in model.named_parameters():\n","#   if 'classifier' not in name: # classifier layer\n","#     assert param.requires_grad == False\n","#   else:\n","#     assert param.requires_grad == True\n","\n","trainer = Trainer(\n","    model=model,                         # the instantiated 🤗 Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=val_dataset             # evaluation dataset\n",")\n","\n","trainer.train()\n","print(f\"Time Elapsed: {time.time() - start_time}\")"],"id":"QK3DHbaq7UPR"},{"cell_type":"markdown","source":["#### F1 Performance Score"],"metadata":{"id":"E917Bla37UPR"},"id":"E917Bla37UPR"},{"cell_type":"code","source":["predictions = trainer.predict(test_dataset)\n","preds = np.argmax(predictions.predictions, axis=-1)\n","f1_metric = evaluate.load(\"f1\")\n","results = f1_metric.compute(predictions=preds, references=predictions.label_ids, average='weighted')\n","print(results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86},"executionInfo":{"status":"ok","timestamp":1667946688837,"user_tz":300,"elapsed":36701,"user":{"displayName":"Ice Gates","userId":"11701491213595641525"}},"outputId":"8904ace0-a745-4b27-cb38-78073dcbb237","id":"GADn6R2P7UPS"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running Prediction *****\n","  Num examples = 1022\n","  Batch size = 64\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'f1': 0.5128169286682006}\n"]}],"id":"GADn6R2P7UPS"},{"cell_type":"markdown","source":["#### Plot Training/Validation Curves"],"metadata":{"id":"lYfETO4G7UPS"},"id":"lYfETO4G7UPS"},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","history = pd.read_excel('Temp.xlsx')\n","plt.plot(history['Step'], history['Training Loss'], label='Training Loss')\n","plt.plot(history['Step'], history['Validation Loss'], label='Validation Loss')\n","\n","plt.title('Loss history')\n","plt.ylabel('Loss')\n","plt.xlabel('Step')\n","plt.legend()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312},"id":"jZy-jHpdXp4r","executionInfo":{"status":"ok","timestamp":1667933415861,"user_tz":300,"elapsed":355,"user":{"displayName":"Justin Xu","userId":"13320263107479278707"}},"outputId":"fcb26a61-b113-4352-d7be-5adbeec0f82b"},"id":"jZy-jHpdXp4r","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.legend.Legend at 0x7f886c34ebd0>"]},"metadata":{},"execution_count":25},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["#### Visualize Explanations\n"],"metadata":{"id":"66rulGuV7UPS"},"id":"66rulGuV7UPS"},{"cell_type":"markdown","source":["Picking the highest predicted value for each class:"],"metadata":{"id":"Oiq1Oxfg7UPS"},"id":"Oiq1Oxfg7UPS"},{"cell_type":"code","source":["id2label = {v: k for k, v in label2id.items()}\n","print(id2label)\n","\n","from transformers_interpret import SequenceClassificationExplainer\n","multiclass_explainer = SequenceClassificationExplainer(model=model, tokenizer=tokenizer)\n","max_predictions = predictions.predictions.max(0)\n","print(max_predictions)\n","\n","examples = {}\n","for each_class in id2label.keys():\n","    examples[each_class] = {}\n","    examples[each_class]['max'] = int(np.where(predictions.predictions[:, each_class] == max_predictions[each_class])[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665769865836,"user_tz":240,"elapsed":15,"user":{"displayName":"Justin Xu","userId":"13320263107479278707"}},"outputId":"9388d018-7f6f-4045-feda-d73224413d1a","id":"tzfyfd9o7UPS"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'No Schooling': 0,\n"," 'Primary School': 1,\n"," 'High School': 2,\n"," 'College or Higher': 3}"]},"metadata":{},"execution_count":12}],"id":"tzfyfd9o7UPS"},{"cell_type":"markdown","source":["Visualizing explanations for the max sample in each class:"],"metadata":{"id":"005_wWc27UPS"},"id":"005_wWc27UPS"},{"cell_type":"code","source":["model.to('cpu')\n","for each_class in examples.keys():\n","    for each_example in examples[each_class].keys():\n","        text_example = test_texts[examples[each_class][each_example]]\n","        word_attributions = multiclass_explainer(text=text_example)\n","        print(f\"Prediction for the {each_example} predicted value example of class {each_class}: {multiclass_explainer.predicted_class_name}\")\n","        html = multiclass_explainer.visualize()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":599},"executionInfo":{"status":"error","timestamp":1665770012524,"user_tz":240,"elapsed":2565,"user":{"displayName":"Justin Xu","userId":"13320263107479278707"}},"outputId":"96985ada-df84-4cff-bcbc-adea8cd24479","id":"nXtBBDXF7UPS"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction for the max predicted value example of class 0: LABEL_0\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_0 (0.82)</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_0</b></text></td><td><text style=\"padding-right:2em\"><b>3.38</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> my                    </font></mark><mark style=\"background-color: hsl(120, 75%, 72%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> mother                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> did                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> not                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> have                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> any                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> kind                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sickness                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> she                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> fell                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> down                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> house                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> we                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> took                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> her                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> immediately                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> hospital                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> place                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> doctor                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> said                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> her                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> b                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> p                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> high                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> told                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> us                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> take                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> her                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> hospital                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##2                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> so                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> immediately                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> we                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> took                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> her                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> hospital                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##2                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> while                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> she                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> receiving                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> treatment                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> there                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> she                    </font></mark><mark style=\"background-color: hsl(120, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> died                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"]},"metadata":{}},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-7f49f276abca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0meach_example\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meach_class\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mtext_example\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meach_class\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meach_example\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mword_attributions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulticlass_explainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_example\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Prediction for the {each_example} predicted value example of class {each_class}: {multiclass_explainer.predicted_class_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulticlass_explainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers_interpret/explainers/text/sequence_classification.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, index, class_name, embedding_type, internal_batch_size, n_steps)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minternal_batch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minternal_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers_interpret/explainers/text/sequence_classification.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, text, index, class_name, embedding_type)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_attributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_attributions\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers_interpret/explainers/text/sequence_classification.py\u001b[0m in \u001b[0;36m_calculate_attributions\u001b[0;34m(self, embeddings, index, class_name)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mref_token_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref_token_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0minternal_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         )\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers_interpret/attributions.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, custom_forward, embeddings, tokens, input_ids, ref_input_ids, sep_id, attention_mask, target, token_type_ids, position_ids, ref_token_type_ids, ref_position_ids, internal_batch_size, n_steps)\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0madditional_forward_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0minternal_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             )\n\u001b[1;32m     64\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/captum/log/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/captum/attr/_core/layer/layer_integrated_gradients.py\u001b[0m in \u001b[0;36mattribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta, attribute_to_layer_input)\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0minternal_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minternal_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             \u001b[0mreturn_convergence_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m         )\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/captum/attr/_core/integrated_gradients.py\u001b[0m in \u001b[0;36mattribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta)\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0madditional_forward_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madditional_forward_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m             )\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/captum/attr/_core/integrated_gradients.py\u001b[0m in \u001b[0;36m_attribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, step_sizes_and_alphas)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaled_features_tpl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mtarget_ind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0madditional_forward_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_additional_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m         )\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/captum/attr/_core/layer/layer_integrated_gradients.py\u001b[0m in \u001b[0;36mgradient_func\u001b[0;34m(forward_fn, inputs, target_ind, additional_forward_args)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m                     output = _run_forward(\n\u001b[0;32m--> 465\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_forward_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m                     )\n\u001b[1;32m    467\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/captum/_utils/common.py\u001b[0m in \u001b[0;36m_run_forward\u001b[0;34m(forward_func, inputs, target, additional_forward_args)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0madditional_forward_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madditional_forward_args\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m         \u001b[0;32melse\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m     )\n\u001b[1;32m    461\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_select_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers_interpret/explainers/text/sequence_classification.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, attention_mask)\u001b[0m\n\u001b[1;32m    175\u001b[0m     ):\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers_interpret/explainer.py\u001b[0m in \u001b[0;36m_get_preds\u001b[0;34m(self, input_ids, token_type_ids, position_ids, attention_mask)\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m             )\n\u001b[1;32m    194\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1559\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1560\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1561\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1562\u001b[0m         )\n\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1022\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m         )\n\u001b[1;32m   1026\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    608\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m                 )\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         )\n\u001b[1;32m    496\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         )\n\u001b[1;32m    428\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relative_key\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relative_key_query\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 14.76 GiB total capacity; 13.20 GiB already allocated; 61.75 MiB free; 13.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"id":"nXtBBDXF7UPS"},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"P0k9eU7i7UPS"},"id":"P0k9eU7i7UPS"},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from sklearn.metrics import accuracy_score, roc_curve\n","\n","fpr, tpr, thresholds = roc_curve(test_labels, best_predictions, pos_label = 1)\n","\n","plt.plot(fpr,tpr)\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operator Characteristic (ROC) Curve for RF')\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"id":"VlrgCz7k0qPA","executionInfo":{"status":"ok","timestamp":1666933659869,"user_tz":240,"elapsed":309,"user":{"displayName":"Justin Xu","userId":"13320263107479278707"}},"outputId":"5cf2b263-78b9-4b76-e927-69a5449d98d1"},"id":"VlrgCz7k0qPA","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["### Predicting Gender - 2 Classes\n"],"metadata":{"id":"BgsagYsIHRIP"},"id":"BgsagYsIHRIP"},{"cell_type":"markdown","metadata":{"id":"iuFg85Y7HRIU"},"source":["#### HuggingFace Dataset\n","Encoding labels:"],"id":"iuFg85Y7HRIU"},{"cell_type":"code","source":["print(f\"Possible genders: {list(df['g5_02'].unique())}\")\n","df[~df['g5_02'].isin(['Male', 'Female'])]\n","df_filtered = df[df['g5_02'].isin(['Male', 'Female'])]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RQ21cwV-X8wW","executionInfo":{"status":"ok","timestamp":1667972162185,"user_tz":300,"elapsed":6,"user":{"displayName":"Bb Win","userId":"04140130207490762069"}},"outputId":"5ffefe5c-9a2f-4279-9d3f-83ed8e65eb82"},"id":"RQ21cwV-X8wW","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Possible genders: ['Male', 'Female', nan]\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IAsFFdqQHRIU"},"outputs":[],"source":["documents = df_filtered['open_response'].tolist()\n","label2id = {\n","    'Male': 0,\n","    'Female': 1\n","}\n","labels = df_filtered['g5_02'].map(label2id).tolist()"],"id":"IAsFFdqQHRIU"},{"cell_type":"markdown","source":["Splitting data into Train/Validation/Test according to 68%/17%/15%:"],"metadata":{"id":"vllLhGIzHRIU"},"id":"vllLhGIzHRIU"},{"cell_type":"code","source":["# create train/test split\n","train_texts, test_texts, train_labels, test_labels = train_test_split(documents, labels, test_size=.15, random_state=8573)\n","\n","# create train/validation split\n","train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2, random_state=3820)"],"metadata":{"id":"Ns3B3b0rHRIU"},"execution_count":null,"outputs":[],"id":"Ns3B3b0rHRIU"},{"cell_type":"markdown","source":["Encoding documents using BERT tokenizer:"],"metadata":{"id":"PiLllKq0HRIV"},"id":"PiLllKq0HRIV"},{"cell_type":"code","source":["# tokenize \n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n","val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n","test_encodings = tokenizer(test_texts, truncation=True, padding=True)"],"metadata":{"id":"_u54n27pHRIV"},"execution_count":null,"outputs":[],"id":"_u54n27pHRIV"},{"cell_type":"markdown","source":["Defining dataset object compatible with HuggingFace: "],"metadata":{"id":"aE4Kw4DwHRIV"},"id":"aE4Kw4DwHRIV"},{"cell_type":"code","source":["# create a class for the dataset -> compatible with huggingface trainer\n","class VerbalAutopsyDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = VerbalAutopsyDataset(train_encodings, train_labels)\n","val_dataset = VerbalAutopsyDataset(val_encodings, val_labels)\n","test_dataset = VerbalAutopsyDataset(test_encodings, test_labels)"],"metadata":{"id":"F6v_ARKIHRIV"},"execution_count":null,"outputs":[],"id":"F6v_ARKIHRIV"},{"cell_type":"markdown","source":["#### Model Tuning\n","Fine-tuning BERT model:"],"metadata":{"id":"2Z3OrRr1HRIV"},"id":"2Z3OrRr1HRIV"},{"cell_type":"code","source":["model"],"metadata":{"id":"Rv-_Z6QGKuBl","executionInfo":{"status":"ok","timestamp":1667960071916,"user_tz":300,"elapsed":234,"user":{"displayName":"Ice Gates","userId":"11701491213595641525"}},"outputId":"adcd3b60-8215-45d8-fb4e-1d740efebc8f","colab":{"base_uri":"https://localhost:8080/"}},"id":"Rv-_Z6QGKuBl","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["start_time = time.time()\n","training_args = TrainingArguments(\n","    output_dir='./results',          # output directory\n","    num_train_epochs=10,              # total number of training epochs\n","    per_device_train_batch_size=16,  # batch size per device during training\n","    per_device_eval_batch_size=64,   # batch size for evaluation\n","    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n","    weight_decay=0.01,               # strength of weight decay\n","    logging_dir='./logs',            # directory for storing logs\n","    logging_steps=100,\n","    learning_rate=2e-5,\n","    evaluation_strategy='steps',\n","    eval_steps=100,\n","    load_best_model_at_end=True,\n",")\n","\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n","\n","trainer = Trainer(\n","    model=model,                         # the instantiated 🤗 Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=val_dataset             # evaluation dataset\n",")\n","\n","trainer.train()\n","print(f\"Time Elapsed: {time.time() - start_time}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":720},"outputId":"1e783854-f2e3-4c51-83b9-c6ebfb240523","id":"bOh3wd5uHRIV","executionInfo":{"status":"error","timestamp":1667972219262,"user_tz":300,"elapsed":21585,"user":{"displayName":"Bb Win","userId":"04140130207490762069"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 4624\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2890\n","  Number of trainable parameters = 109483778\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='9' max='2890' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [   9/2890 00:09 < 1:02:51, 0.76 it/s, Epoch 0.03/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-08326e7dca19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Time Elapsed: {time.time() - start_time}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1503\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         )\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1747\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m                 if (\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2524\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2525\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2526\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2528\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"id":"bOh3wd5uHRIV"},{"cell_type":"markdown","source":["Freezing all but final classifier layer in BERT:"],"metadata":{"id":"R6apRwvMHRIV"},"id":"R6apRwvMHRIV"},{"cell_type":"code","execution_count":null,"metadata":{"id":"f2XlkJlqHRIV","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1667974862192,"user_tz":300,"elapsed":2637912,"user":{"displayName":"Bb Win","userId":"04140130207490762069"}},"outputId":"192e2040-84ee-44ca-f022-69535c8c6fb8"},"outputs":[{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/config.json\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.24.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/pytorch_model.bin\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["bert.embeddings.word_embeddings.weight False\n","bert.embeddings.position_embeddings.weight False\n","bert.embeddings.token_type_embeddings.weight False\n","bert.embeddings.LayerNorm.weight False\n","bert.embeddings.LayerNorm.bias False\n","bert.encoder.layer.0.attention.self.query.weight False\n","bert.encoder.layer.0.attention.self.query.bias False\n","bert.encoder.layer.0.attention.self.key.weight False\n","bert.encoder.layer.0.attention.self.key.bias False\n","bert.encoder.layer.0.attention.self.value.weight False\n","bert.encoder.layer.0.attention.self.value.bias False\n","bert.encoder.layer.0.attention.output.dense.weight False\n","bert.encoder.layer.0.attention.output.dense.bias False\n","bert.encoder.layer.0.attention.output.LayerNorm.weight False\n","bert.encoder.layer.0.attention.output.LayerNorm.bias False\n","bert.encoder.layer.0.intermediate.dense.weight False\n","bert.encoder.layer.0.intermediate.dense.bias False\n","bert.encoder.layer.0.output.dense.weight False\n","bert.encoder.layer.0.output.dense.bias False\n","bert.encoder.layer.0.output.LayerNorm.weight False\n","bert.encoder.layer.0.output.LayerNorm.bias False\n","bert.encoder.layer.1.attention.self.query.weight False\n","bert.encoder.layer.1.attention.self.query.bias False\n","bert.encoder.layer.1.attention.self.key.weight False\n","bert.encoder.layer.1.attention.self.key.bias False\n","bert.encoder.layer.1.attention.self.value.weight False\n","bert.encoder.layer.1.attention.self.value.bias False\n","bert.encoder.layer.1.attention.output.dense.weight False\n","bert.encoder.layer.1.attention.output.dense.bias False\n","bert.encoder.layer.1.attention.output.LayerNorm.weight False\n","bert.encoder.layer.1.attention.output.LayerNorm.bias False\n","bert.encoder.layer.1.intermediate.dense.weight False\n","bert.encoder.layer.1.intermediate.dense.bias False\n","bert.encoder.layer.1.output.dense.weight False\n","bert.encoder.layer.1.output.dense.bias False\n","bert.encoder.layer.1.output.LayerNorm.weight False\n","bert.encoder.layer.1.output.LayerNorm.bias False\n","bert.encoder.layer.2.attention.self.query.weight False\n","bert.encoder.layer.2.attention.self.query.bias False\n","bert.encoder.layer.2.attention.self.key.weight False\n","bert.encoder.layer.2.attention.self.key.bias False\n","bert.encoder.layer.2.attention.self.value.weight False\n","bert.encoder.layer.2.attention.self.value.bias False\n","bert.encoder.layer.2.attention.output.dense.weight False\n","bert.encoder.layer.2.attention.output.dense.bias False\n","bert.encoder.layer.2.attention.output.LayerNorm.weight False\n","bert.encoder.layer.2.attention.output.LayerNorm.bias False\n","bert.encoder.layer.2.intermediate.dense.weight False\n","bert.encoder.layer.2.intermediate.dense.bias False\n","bert.encoder.layer.2.output.dense.weight False\n","bert.encoder.layer.2.output.dense.bias False\n","bert.encoder.layer.2.output.LayerNorm.weight False\n","bert.encoder.layer.2.output.LayerNorm.bias False\n","bert.encoder.layer.3.attention.self.query.weight False\n","bert.encoder.layer.3.attention.self.query.bias False\n","bert.encoder.layer.3.attention.self.key.weight False\n","bert.encoder.layer.3.attention.self.key.bias False\n","bert.encoder.layer.3.attention.self.value.weight False\n","bert.encoder.layer.3.attention.self.value.bias False\n","bert.encoder.layer.3.attention.output.dense.weight False\n","bert.encoder.layer.3.attention.output.dense.bias False\n","bert.encoder.layer.3.attention.output.LayerNorm.weight False\n","bert.encoder.layer.3.attention.output.LayerNorm.bias False\n","bert.encoder.layer.3.intermediate.dense.weight False\n","bert.encoder.layer.3.intermediate.dense.bias False\n","bert.encoder.layer.3.output.dense.weight False\n","bert.encoder.layer.3.output.dense.bias False\n","bert.encoder.layer.3.output.LayerNorm.weight False\n","bert.encoder.layer.3.output.LayerNorm.bias False\n","bert.encoder.layer.4.attention.self.query.weight False\n","bert.encoder.layer.4.attention.self.query.bias False\n","bert.encoder.layer.4.attention.self.key.weight False\n","bert.encoder.layer.4.attention.self.key.bias False\n","bert.encoder.layer.4.attention.self.value.weight False\n","bert.encoder.layer.4.attention.self.value.bias False\n","bert.encoder.layer.4.attention.output.dense.weight False\n","bert.encoder.layer.4.attention.output.dense.bias False\n","bert.encoder.layer.4.attention.output.LayerNorm.weight False\n","bert.encoder.layer.4.attention.output.LayerNorm.bias False\n","bert.encoder.layer.4.intermediate.dense.weight False\n","bert.encoder.layer.4.intermediate.dense.bias False\n","bert.encoder.layer.4.output.dense.weight False\n","bert.encoder.layer.4.output.dense.bias False\n","bert.encoder.layer.4.output.LayerNorm.weight False\n","bert.encoder.layer.4.output.LayerNorm.bias False\n","bert.encoder.layer.5.attention.self.query.weight False\n","bert.encoder.layer.5.attention.self.query.bias False\n","bert.encoder.layer.5.attention.self.key.weight False\n","bert.encoder.layer.5.attention.self.key.bias False\n","bert.encoder.layer.5.attention.self.value.weight False\n","bert.encoder.layer.5.attention.self.value.bias False\n","bert.encoder.layer.5.attention.output.dense.weight False\n","bert.encoder.layer.5.attention.output.dense.bias False\n","bert.encoder.layer.5.attention.output.LayerNorm.weight False\n","bert.encoder.layer.5.attention.output.LayerNorm.bias False\n","bert.encoder.layer.5.intermediate.dense.weight False\n","bert.encoder.layer.5.intermediate.dense.bias False\n","bert.encoder.layer.5.output.dense.weight False\n","bert.encoder.layer.5.output.dense.bias False\n","bert.encoder.layer.5.output.LayerNorm.weight False\n","bert.encoder.layer.5.output.LayerNorm.bias False\n","bert.encoder.layer.6.attention.self.query.weight False\n","bert.encoder.layer.6.attention.self.query.bias False\n","bert.encoder.layer.6.attention.self.key.weight False\n","bert.encoder.layer.6.attention.self.key.bias False\n","bert.encoder.layer.6.attention.self.value.weight False\n","bert.encoder.layer.6.attention.self.value.bias False\n","bert.encoder.layer.6.attention.output.dense.weight False\n","bert.encoder.layer.6.attention.output.dense.bias False\n","bert.encoder.layer.6.attention.output.LayerNorm.weight False\n","bert.encoder.layer.6.attention.output.LayerNorm.bias False\n","bert.encoder.layer.6.intermediate.dense.weight False\n","bert.encoder.layer.6.intermediate.dense.bias False\n","bert.encoder.layer.6.output.dense.weight False\n","bert.encoder.layer.6.output.dense.bias False\n","bert.encoder.layer.6.output.LayerNorm.weight False\n","bert.encoder.layer.6.output.LayerNorm.bias False\n","bert.encoder.layer.7.attention.self.query.weight False\n","bert.encoder.layer.7.attention.self.query.bias False\n","bert.encoder.layer.7.attention.self.key.weight False\n","bert.encoder.layer.7.attention.self.key.bias False\n","bert.encoder.layer.7.attention.self.value.weight False\n","bert.encoder.layer.7.attention.self.value.bias False\n","bert.encoder.layer.7.attention.output.dense.weight False\n","bert.encoder.layer.7.attention.output.dense.bias False\n","bert.encoder.layer.7.attention.output.LayerNorm.weight False\n","bert.encoder.layer.7.attention.output.LayerNorm.bias False\n","bert.encoder.layer.7.intermediate.dense.weight False\n","bert.encoder.layer.7.intermediate.dense.bias False\n","bert.encoder.layer.7.output.dense.weight False\n","bert.encoder.layer.7.output.dense.bias False\n","bert.encoder.layer.7.output.LayerNorm.weight False\n","bert.encoder.layer.7.output.LayerNorm.bias False\n","bert.encoder.layer.8.attention.self.query.weight False\n","bert.encoder.layer.8.attention.self.query.bias False\n","bert.encoder.layer.8.attention.self.key.weight False\n","bert.encoder.layer.8.attention.self.key.bias False\n","bert.encoder.layer.8.attention.self.value.weight False\n","bert.encoder.layer.8.attention.self.value.bias False\n","bert.encoder.layer.8.attention.output.dense.weight False\n","bert.encoder.layer.8.attention.output.dense.bias False\n","bert.encoder.layer.8.attention.output.LayerNorm.weight False\n","bert.encoder.layer.8.attention.output.LayerNorm.bias False\n","bert.encoder.layer.8.intermediate.dense.weight False\n","bert.encoder.layer.8.intermediate.dense.bias False\n","bert.encoder.layer.8.output.dense.weight False\n","bert.encoder.layer.8.output.dense.bias False\n","bert.encoder.layer.8.output.LayerNorm.weight False\n","bert.encoder.layer.8.output.LayerNorm.bias False\n","bert.encoder.layer.9.attention.self.query.weight False\n","bert.encoder.layer.9.attention.self.query.bias False\n","bert.encoder.layer.9.attention.self.key.weight False\n","bert.encoder.layer.9.attention.self.key.bias False\n","bert.encoder.layer.9.attention.self.value.weight False\n","bert.encoder.layer.9.attention.self.value.bias False\n","bert.encoder.layer.9.attention.output.dense.weight False\n","bert.encoder.layer.9.attention.output.dense.bias False\n","bert.encoder.layer.9.attention.output.LayerNorm.weight False\n","bert.encoder.layer.9.attention.output.LayerNorm.bias False\n","bert.encoder.layer.9.intermediate.dense.weight False\n","bert.encoder.layer.9.intermediate.dense.bias False\n","bert.encoder.layer.9.output.dense.weight False\n","bert.encoder.layer.9.output.dense.bias False\n","bert.encoder.layer.9.output.LayerNorm.weight False\n","bert.encoder.layer.9.output.LayerNorm.bias False\n","bert.encoder.layer.10.attention.self.query.weight False\n","bert.encoder.layer.10.attention.self.query.bias False\n","bert.encoder.layer.10.attention.self.key.weight False\n","bert.encoder.layer.10.attention.self.key.bias False\n","bert.encoder.layer.10.attention.self.value.weight False\n","bert.encoder.layer.10.attention.self.value.bias False\n","bert.encoder.layer.10.attention.output.dense.weight False\n","bert.encoder.layer.10.attention.output.dense.bias False\n","bert.encoder.layer.10.attention.output.LayerNorm.weight False\n","bert.encoder.layer.10.attention.output.LayerNorm.bias False\n","bert.encoder.layer.10.intermediate.dense.weight False\n","bert.encoder.layer.10.intermediate.dense.bias False\n","bert.encoder.layer.10.output.dense.weight False\n","bert.encoder.layer.10.output.dense.bias False\n","bert.encoder.layer.10.output.LayerNorm.weight False\n","bert.encoder.layer.10.output.LayerNorm.bias False\n","bert.encoder.layer.11.attention.self.query.weight False\n","bert.encoder.layer.11.attention.self.query.bias False\n","bert.encoder.layer.11.attention.self.key.weight False\n","bert.encoder.layer.11.attention.self.key.bias False\n","bert.encoder.layer.11.attention.self.value.weight False\n","bert.encoder.layer.11.attention.self.value.bias False\n","bert.encoder.layer.11.attention.output.dense.weight False\n","bert.encoder.layer.11.attention.output.dense.bias False\n","bert.encoder.layer.11.attention.output.LayerNorm.weight False\n","bert.encoder.layer.11.attention.output.LayerNorm.bias False\n","bert.encoder.layer.11.intermediate.dense.weight False\n","bert.encoder.layer.11.intermediate.dense.bias False\n","bert.encoder.layer.11.output.dense.weight False\n","bert.encoder.layer.11.output.dense.bias False\n","bert.encoder.layer.11.output.LayerNorm.weight False\n","bert.encoder.layer.11.output.LayerNorm.bias False\n","bert.pooler.dense.weight False\n","bert.pooler.dense.bias False\n","classifier.weight True\n","classifier.bias True\n"]},{"output_type":"stream","name":"stderr","text":["***** Running training *****\n","  Num examples = 4624\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2890\n","  Number of trainable parameters = 1538\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2890' max='2890' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2890/2890 43:54, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>0.702900</td>\n","      <td>0.693908</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.691700</td>\n","      <td>0.692910</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.689300</td>\n","      <td>0.691356</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.685400</td>\n","      <td>0.688838</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.687700</td>\n","      <td>0.683921</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.681400</td>\n","      <td>0.683953</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.673900</td>\n","      <td>0.685982</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.672000</td>\n","      <td>0.679145</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.674800</td>\n","      <td>0.674348</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.670600</td>\n","      <td>0.676338</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>0.674900</td>\n","      <td>0.675780</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.667800</td>\n","      <td>0.673459</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>0.662800</td>\n","      <td>0.674715</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.667900</td>\n","      <td>0.672311</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.660000</td>\n","      <td>0.667551</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.661700</td>\n","      <td>0.669148</td>\n","    </tr>\n","    <tr>\n","      <td>1700</td>\n","      <td>0.658800</td>\n","      <td>0.672720</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>0.663000</td>\n","      <td>0.664747</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.653900</td>\n","      <td>0.667562</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.663700</td>\n","      <td>0.663187</td>\n","    </tr>\n","    <tr>\n","      <td>2100</td>\n","      <td>0.655800</td>\n","      <td>0.663171</td>\n","    </tr>\n","    <tr>\n","      <td>2200</td>\n","      <td>0.653400</td>\n","      <td>0.662578</td>\n","    </tr>\n","    <tr>\n","      <td>2300</td>\n","      <td>0.657100</td>\n","      <td>0.661614</td>\n","    </tr>\n","    <tr>\n","      <td>2400</td>\n","      <td>0.655900</td>\n","      <td>0.661550</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.652200</td>\n","      <td>0.661769</td>\n","    </tr>\n","    <tr>\n","      <td>2600</td>\n","      <td>0.654600</td>\n","      <td>0.661628</td>\n","    </tr>\n","    <tr>\n","      <td>2700</td>\n","      <td>0.661000</td>\n","      <td>0.660820</td>\n","    </tr>\n","    <tr>\n","      <td>2800</td>\n","      <td>0.651600</td>\n","      <td>0.660615</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1156\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1156\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1156\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1156\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1156\n","  Batch size = 64\n","Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 1156\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1156\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1156\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1156\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1156\n","  Batch size = 64\n","Saving model checkpoint to ./results/checkpoint-1000\n","Configuration saved in ./results/checkpoint-1000/config.json\n","Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 1156\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1156\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1156\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1156\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1156\n","  Batch size = 64\n","Saving model checkpoint to ./results/checkpoint-1500\n","Configuration saved in ./results/checkpoint-1500/config.json\n","Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 1156\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1156\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1156\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1156\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1156\n","  Batch size = 64\n","Saving model checkpoint to ./results/checkpoint-2000\n","Configuration saved in ./results/checkpoint-2000/config.json\n","Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 1156\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1156\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1156\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1156\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1156\n","  Batch size = 64\n","Saving model checkpoint to ./results/checkpoint-2500\n","Configuration saved in ./results/checkpoint-2500/config.json\n","Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 1156\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1156\n","  Batch size = 64\n","***** Running Evaluation *****\n","  Num examples = 1156\n","  Batch size = 64\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from ./results/checkpoint-2500 (score: 0.6617688536643982).\n"]},{"output_type":"stream","name":"stdout","text":["Time Elapsed: 2637.6054966449738\n"]}],"source":["start_time = time.time()\n","training_args = TrainingArguments(\n","    output_dir='./results',          # output directory\n","    num_train_epochs=10,              # total number of training epochs\n","    per_device_train_batch_size=16,  # batch size per device during training\n","    per_device_eval_batch_size=64,   # batch size for evaluation\n","    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n","    weight_decay=0.01,               # strength of weight decay\n","    logging_dir='./logs',            # directory for storing logs\n","    logging_steps=100,\n","    learning_rate=2e-5,\n","    evaluation_strategy='steps',\n","    eval_steps=100,\n","    load_best_model_at_end=True,\n",")\n","\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n","\n","# freezing all but final classifier layer\n","for name, param in model.named_parameters():\n","\tif 'classifier' not in name: # classifier layer\n","\t\tparam.requires_grad = False\n","\n","for name, param in model.named_parameters():\n","     print(name, param.requires_grad)\n","\n","trainer = Trainer(\n","    model=model,                         # the instantiated 🤗 Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=val_dataset             # evaluation dataset\n",")\n","\n","trainer.train()\n","print(f\"Time Elapsed: {time.time() - start_time}\")"],"id":"f2XlkJlqHRIV"},{"cell_type":"markdown","source":["#### F1 Performance Score"],"metadata":{"id":"CGjIpmrIHRIV"},"id":"CGjIpmrIHRIV"},{"cell_type":"code","source":["predictions = trainer.predict(test_dataset)\n","preds = np.argmax(predictions.predictions, axis=-1)\n","f1_metric = evaluate.load(\"f1\")\n","results = f1_metric.compute(predictions=preds, references=predictions.label_ids, average='weighted')\n","print(results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86},"id":"akR-7BZVCD3b","executionInfo":{"status":"ok","timestamp":1667959563696,"user_tz":300,"elapsed":37931,"user":{"displayName":"Ice Gates","userId":"11701491213595641525"}},"outputId":"2ec394b2-49e8-4552-e977-1b8d538a6263"},"id":"akR-7BZVCD3b","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running Prediction *****\n","  Num examples = 1020\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'f1': 0.6218308697088384}\n"]}]},{"cell_type":"code","source":["predictions = trainer.predict(test_dataset)\n","preds = np.argmax(predictions.predictions, axis=-1)\n","f1_metric = evaluate.load(\"f1\")\n","results = f1_metric.compute(predictions=preds, references=predictions.label_ids, average='weighted')\n","print(results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":478},"executionInfo":{"status":"ok","timestamp":1667948882269,"user_tz":300,"elapsed":36412,"user":{"displayName":"Ice Gates","userId":"11701491213595641525"}},"outputId":"e16de574-e757-4dd1-cba8-16b68741e868","id":"I5bcuLXoHRIV"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running Prediction *****\n","  Num examples = 1020\n","  Batch size = 64\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1085' max='2890' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1085/2890 34:00 < 56:41, 0.53 it/s, Epoch 3.75/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>0.640200</td>\n","      <td>0.485335</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.448400</td>\n","      <td>0.466381</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.420900</td>\n","      <td>0.417709</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.394300</td>\n","      <td>0.386431</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.413200</td>\n","      <td>0.425894</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.396500</td>\n","      <td>0.378334</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.385900</td>\n","      <td>0.383508</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.370400</td>\n","      <td>0.410797</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.351500</td>\n","      <td>0.492856</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.330300</td>\n","      <td>0.433072</td>\n","    </tr>\n","  </tbody>\n","</table><p>\n","    <div>\n","      \n","      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [16/16 00:33]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'f1': 0.788472787429817}\n"]}],"id":"I5bcuLXoHRIV"},{"cell_type":"markdown","source":["#### Plot Training/Validation Curves"],"metadata":{"id":"S6-qbYJpHRIV"},"id":"S6-qbYJpHRIV"},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","history = pd.read_excel('Temp.xlsx')\n","plt.plot(history['Step'], history['Training Loss'], label='Training Loss')\n","plt.plot(history['Step'], history['Validation Loss'], label='Validation Loss')\n","\n","plt.title('Loss history')\n","plt.ylabel('Loss')\n","plt.xlabel('Step')\n","plt.legend()"],"metadata":{"id":"fTf1RzjFgX-c","colab":{"base_uri":"https://localhost:8080/","height":312},"executionInfo":{"status":"ok","timestamp":1667965089522,"user_tz":300,"elapsed":219,"user":{"displayName":"Bb Win","userId":"04140130207490762069"}},"outputId":"b4b8becd-d652-41a0-e4cb-eb80420acae5"},"id":"fTf1RzjFgX-c","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.legend.Legend at 0x7fb08ab87f10>"]},"metadata":{},"execution_count":19},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["#### Visualize Explanations\n"],"metadata":{"id":"5IAm3OnOHRIV"},"id":"5IAm3OnOHRIV"},{"cell_type":"markdown","source":["Picking the highest predicted value for each class:"],"metadata":{"id":"ZeKdZ3dgHRIV"},"id":"ZeKdZ3dgHRIV"},{"cell_type":"code","source":["import numpy as np\n","def find_nearest(array, value):\n","    array = np.asarray(array)\n","    idx = (np.abs(array - value)).argmin()\n","    return array[idx]\n","\n","print(find_nearest(predictions.predictions[:, 0], value=0))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d9951JQ70C4s","executionInfo":{"status":"ok","timestamp":1667975058051,"user_tz":300,"elapsed":231,"user":{"displayName":"Bb Win","userId":"04140130207490762069"}},"outputId":"1499f423-c2b4-4fff-c72d-293b2bdad0aa"},"id":"d9951JQ70C4s","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.0001351205\n"]}]},{"cell_type":"code","source":["id2label = {v: k for k, v in label2id.items()}\n","print(id2label)\n","\n","from transformers_interpret import SequenceClassificationExplainer\n","multiclass_explainer = SequenceClassificationExplainer(model=model, tokenizer=tokenizer)\n","\n","examples = {}\n","for each_class in id2label.keys():\n","    max_prediction = find_nearest(predictions.predictions[:, each_class], value=each_class)\n","    print(max_prediction)\n","    examples[each_class] = {}\n","    examples[each_class]['max'] = int(np.where(predictions.predictions[:, each_class] == max_prediction)[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667975265298,"user_tz":300,"elapsed":203,"user":{"displayName":"Bb Win","userId":"04140130207490762069"}},"outputId":"e6af1854-46e2-41fa-a22c-eb59737e675e","id":"4C0eHlQiHRIW"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: 'Male', 1: 'Female'}\n","0.0001351205\n","0.33292553\n"]}],"id":"4C0eHlQiHRIW"},{"cell_type":"markdown","source":["Visualizing explanations for the max sample in each class:"],"metadata":{"id":"uu_2c_1DHRIW"},"id":"uu_2c_1DHRIW"},{"cell_type":"code","source":["model.to('cuda')\n","for each_class in examples.keys():\n","    for each_example in examples[each_class].keys():\n","        text_example = test_texts[examples[each_class][each_example]]\n","        word_attributions = multiclass_explainer(text=text_example)\n","        print(f\"Prediction for the {each_example} predicted value example of class {each_class}: {multiclass_explainer.predicted_class_name}\")\n","        html = multiclass_explainer.visualize()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":257},"executionInfo":{"status":"ok","timestamp":1667975204869,"user_tz":300,"elapsed":1002,"user":{"displayName":"Bb Win","userId":"04140130207490762069"}},"outputId":"621040b2-f59c-40a0-ff3a-a5e4a7739a7d","id":"m9bCGK_hHRIW"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction for the max predicted value example of class 0: LABEL_1\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_1 (0.57)</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_1</b></text></td><td><text style=\"padding-right:2em\"><b>2.32</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 61%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> client                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> thanked                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> services                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> which                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> provided                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> by                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> nurses                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> doctors                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> hospital                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Prediction for the max predicted value example of class 1: LABEL_1\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_1 (0.52)</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_1</b></text></td><td><text style=\"padding-right:2em\"><b>0.98</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> participant                    </font></mark><mark style=\"background-color: hsl(120, 75%, 68%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> satisfied                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> with                    </font></mark><mark style=\"background-color: hsl(0, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> services                    </font></mark><mark style=\"background-color: hsl(120, 75%, 78%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> provided                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> at                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> hospital                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"]},"metadata":{}}],"id":"m9bCGK_hHRIW"},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"u1I23yZ9HRIW"},"id":"u1I23yZ9HRIW"},{"cell_type":"markdown","source":["# Baseline TF-IDF Model\n"],"metadata":{"id":"PeoEUwJQxYvd"},"id":"PeoEUwJQxYvd"},{"cell_type":"markdown","source":["### Imports & Setup"],"metadata":{"id":"H2zWfqOuDL4t"},"id":"H2zWfqOuDL4t"},{"cell_type":"code","source":["import numpy as np\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import classification_report, precision_recall_fscore_support\n","from nltk.stem.snowball import EnglishStemmer"],"metadata":{"id":"8g_lQ8bixe34"},"id":"8g_lQ8bixe34","execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"-IhDqfG6xl3k"},"id":"-IhDqfG6xl3k","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Classification of Education Level - 4 Classes\n","Splitting data and encoding labels:"],"metadata":{"id":"b_9J8gl8DS6H"},"id":"b_9J8gl8DS6H"},{"cell_type":"code","source":["documents = df['open_response'].tolist()\n","label2id = {\n","    'No Schooling': 0,\n","    'Primary School': 1,\n","    'High School': 2,\n","    'College or Higher': 3\n","}\n","labels = df['g5_06a'].map(label2id).tolist()\n","\n","# create train/test split\n","train_texts, test_texts, train_labels, test_labels = train_test_split(documents, labels, test_size=.15, random_state=8573)\n","\n","# create train/validation split\n","train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2, random_state=3820)"],"metadata":{"id":"aSsoM4zgEUwE"},"id":"aSsoM4zgEUwE","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Defining analyzer with stemmer:"],"metadata":{"id":"oHWRLaDMGZj3"},"id":"oHWRLaDMGZj3"},{"cell_type":"code","source":["stemmer = EnglishStemmer()\n","analyzer = CountVectorizer().build_analyzer()\n","def stemmed_words(doc):\n","    return (stemmer.stem(w) for w in analyzer(doc))"],"metadata":{"id":"EQ2TIiuLDJPN"},"id":"EQ2TIiuLDJPN","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Defining NLP pipeline and grid search parameters, fitting model:"],"metadata":{"id":"BZWH7sr3GdDK"},"id":"BZWH7sr3GdDK"},{"cell_type":"code","source":["pipeline = Pipeline([\n","    ('vectorizer', CountVectorizer(analyzer=stemmed_words)),\n","    ('tfidf', TfidfTransformer()),\n","    ('multi_regression', LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter = 100000)) #one vs rest\n","])\n","\n","parameters = {'multi_regression__C': [0.01, 0.1, 0.5, 1, 2, 5, 10, 100, 1000], # other parameters? inverse of l2 regularization, larger C more prone to overfit\n","              'vectorizer__stop_words': ['english'],\n","              'vectorizer__max_features': [500]}\n","\n","pipeline_gridsearch_cv = GridSearchCV(pipeline, parameters, cv=5, verbose=1)\n","pipeline_gridsearch_cv.fit(train_texts, train_labels)\n","best_predictions = pipeline_gridsearch_cv.predict(test_texts)\n","\n","baseline_accuracy = np.mean(best_predictions == test_labels)\n","print(\"TF-IDF Accuracy:\", baseline_accuracy)\n","print(classification_report(test_labels, best_predictions))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dk4ArG99ymZJ","executionInfo":{"status":"ok","timestamp":1667970135107,"user_tz":300,"elapsed":185506,"user":{"displayName":"Bb Win","userId":"04140130207490762069"}},"outputId":"b61dd921-14a3-4ce3-b7f7-3ad6b4fd31e3"},"id":"dk4ArG99ymZJ","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 5 folds for each of 9 candidates, totalling 45 fits\n","TF-IDF Accuracy: 0.45499021526418787\n","              precision    recall  f1-score   support\n","\n","           0       0.44      0.29      0.35       215\n","           1       0.46      0.88      0.61       447\n","           2       0.21      0.03      0.05       206\n","           3       0.80      0.03      0.05       154\n","\n","    accuracy                           0.45      1022\n","   macro avg       0.48      0.31      0.26      1022\n","weighted avg       0.46      0.45      0.36      1022\n","\n"]}]},{"cell_type":"markdown","source":["Printing feature words:"],"metadata":{"id":"FvTCcly_Gln-"},"id":"FvTCcly_Gln-"},{"cell_type":"code","source":["with np.printoptions(threshold=np.inf):\n","    print(pipeline_gridsearch_cv.best_estimator_.named_steps['vectorizer'].get_feature_names_out())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zR48eYxf8mpX","executionInfo":{"status":"ok","timestamp":1667933658780,"user_tz":300,"elapsed":332,"user":{"displayName":"Justin Xu","userId":"13320263107479278707"}},"outputId":"d6218aa4-6535-4ded-f431-3bcb11668fc1"},"id":"zR48eYxf8mpX","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['00' '10' '11' '12' '15' '20' '2007' '2008' '2009' '30' '_x000d_'\n"," '_x000d__x000d_' 'abdomen' 'abdomin' 'abl' 'about' 'accid' 'accord' 'add'\n"," 'addit' 'admit' 'advis' 'affect' 'after' 'afternoon' 'again' 'age' 'ago'\n"," 'aid' 'alcohol' 'all' 'alreadi' 'also' 'alway' 'am' 'amount' 'an' 'and'\n"," 'ani' 'anoth' 'answer' 'anymor' 'anyth' 'appear' 'appetit' 'april' 'are'\n"," 'around' 'arriv' 'as' 'ask' 'asthma' 'at' 'attack' 'attent' 'away' 'babi'\n"," 'back' 'bad' 'be' 'becam' 'becaus' 'becom' 'bed' 'been' 'befor' 'began'\n"," 'better' 'birth' 'bleed' 'blood' 'bodi' 'both' 'bp' 'brain' 'breast'\n"," 'breath' 'bring' 'brother' 'brought' 'burn' 'but' 'by' 'call' 'came'\n"," 'can' 'cancer' 'cannot' 'care' 'caus' 'certif' 'chang' 'check'\n"," 'chemotherapi' 'chest' 'child' 'client' 'clot' 'cold' 'color' 'coma'\n"," 'come' 'comment' 'complain' 'complet' 'condit' 'confin' 'conscious'\n"," 'consult' 'consum' 'continu' 'control' 'convuls' 'cough' 'could' 'ct'\n"," 'cure' 'damag' 'date' 'daughter' 'day' 'dead' 'death' 'deceas' 'decemb'\n"," 'decid' 'deliveri' 'diabet' 'diagnos' 'dialysi' 'did' 'didn' 'die'\n"," 'difficult' 'difficulti' 'discharg' 'diseas' 'do' 'doctor' 'document'\n"," 'doe' 'don' 'done' 'down' 'drink' 'drug' 'due' 'dure' 'earlier' 'eat'\n"," 'edema' 'edemat' 'emerg' 'emot' 'enlarg' 'especi' 'even' 'everi' 'examin'\n"," 'excret' 'experi' 'experienc' 'expir' 'explain' 'eye' 'face' 'fail'\n"," 'fall' 'famili' 'father' 'feel' 'feet' 'fell' 'felt' 'fever' 'few'\n"," 'final' 'find' 'fine' 'first' 'fluid' 'follow' 'food' 'foot' 'for' 'form'\n"," 'found' 'from' 'further' 'gave' 'get' 'give' 'given' 'glucos' 'go' 'good'\n"," 'got' 'gotten' 'had' 'hand' 'happen' 'has' 'have' 'he' 'head' 'headach'\n"," 'health' 'healthi' 'heart' 'help' 'hemorrhag' 'her' 'here' 'high' 'him'\n"," 'his' 'hit' 'hiv' 'home' 'hospit' 'hospital2' 'hospital3' 'hospital4'\n"," 'hour' 'hous' 'how' 'husband' 'icu' 'if' 'ill' 'immedi' 'in' 'increas'\n"," 'infect' 'inform' 'inject' 'insert' 'insid' 'interview' 'interviewe'\n"," 'intestin' 'into' 'is' 'it' 'januari' 'jaundic' 'juli' 'june' 'just'\n"," 'kept' 'kidney' 'kind' 'know' 'lack' 'ladi' 'last' 'late' 'later' 'law'\n"," 'left' 'leg' 'level' 'like' 'littl' 'live' 'liver' 'long' 'look' 'loos'\n"," 'loss' 'lost' 'lot' 'low' 'lower' 'lump' 'lung' 'made' 'mani' 'march'\n"," 'may' 'mayb' 'me' 'med' 'medic' 'medicin' 'member' 'mention' 'minut'\n"," 'misplac' 'money' 'month' 'more' 'morn' 'mother' 'motion' 'mouth' 'move'\n"," 'much' 'my' 'neck' 'need' 'never' 'next' 'night' 'no' 'normal' 'not'\n"," 'note' 'noth' 'notic' 'nurs' 'observ' 'occur' 'of' 'off' 'often' 'old'\n"," 'on' 'onc' 'one' 'onli' 'oper' 'or' 'other' 'our' 'out' 'over' 'oxygen'\n"," 'pain' 'paralysi' 'part' 'particip' 'pass' 'patient' 'peopl' 'perform'\n"," 'person' 'place' 'place2' 'pm' 'pneumonia' 'point' 'pregnant' 'prescrib'\n"," 'pressur' 'prior' 'privat' 'problem' 'proper' 'provid' 'put' 'question'\n"," 'ray' 'reach' 'realli' 'reason' 'receiv' 'record' 'recov' 'reduc' 'refer'\n"," 'refus' 'regular' 'relat' 'reliev' 'remain' 'remov' 'report' 'respond'\n"," 'result' 'right' 'room' 'rush' 'said' 'same' 'saw' 'say' 'scan' 'second'\n"," 'see' 'seem' 'sent' 'serious' 'servic' 'sever' 'she' 'should' 'show'\n"," 'shown' 'sick' 'side' 'sinc' 'sister' 'sleep' 'small' 'smoke' 'smooth'\n"," 'so' 'some' 'someth' 'sometim' 'son' 'speak' 'start' 'stay' 'still'\n"," 'stomach' 'stool' 'stop' 'stroke' 'sudden' 'suffer' 'sugar' 'suggest'\n"," 'support' 'suppos' 'surgeri' 'surviv' 'swell' 'take' 'taken' 'talk' 'tb'\n"," 'tell' 'test' 'than' 'thank' 'that' 'the' 'their' 'them' 'then' 'there'\n"," 'they' 'thing' 'think' 'this' 'those' 'thought' 'three' 'through' 'till'\n"," 'time' 'to' 'told' 'too' 'took' 'transfer' 'treat' 'treatment' 'tri'\n"," 'tube' 'tuberculosi' 'tumor' 'turn' 'two' 'ulcer' 'unabl' 'unconsci'\n"," 'under' 'undergo' 'undergon' 'until' 'up' 'urin' 'us' 'use' 'veri'\n"," 'visit' 'vomit' 'walk' 'want' 'was' 'water' 'way' 'we' 'weak' 'week'\n"," 'weight' 'well' 'went' 'were' 'what' 'when' 'whenev' 'where' 'whi'\n"," 'which' 'while' 'who' 'whole' 'wife' 'will' 'with' 'without' 'work'\n"," 'worst' 'would' 'wound' 'year']\n"]}]},{"cell_type":"markdown","source":["### Classification of Education Level - 2 Classes (Binarized)\n","Splitting data and encoding labels:"],"metadata":{"id":"LQ3Vn-1VGr1e"},"id":"LQ3Vn-1VGr1e"},{"cell_type":"code","source":["documents = df['open_response'].tolist()\n","label2id = {\n","    'No Schooling': 0,\n","    'Primary School': 0,\n","    'High School': 1,\n","    'College or Higher': 1\n","}\n","labels = df['g5_06a'].map(label2id).tolist()\n","\n","# create train/test split\n","train_texts, test_texts, train_labels, test_labels = train_test_split(documents, labels, test_size=.15, random_state=8573)\n","\n","# create train/validation split\n","train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2, random_state=3820)"],"metadata":{"id":"fSvAYSd5Gr1e"},"execution_count":null,"outputs":[],"id":"fSvAYSd5Gr1e"},{"cell_type":"markdown","source":["Defining analyzer with stemmer:"],"metadata":{"id":"HVTF1aAzGr1e"},"id":"HVTF1aAzGr1e"},{"cell_type":"code","source":["stemmer = EnglishStemmer()\n","analyzer = CountVectorizer().build_analyzer()\n","def stemmed_words(doc):\n","    return (stemmer.stem(w) for w in analyzer(doc))"],"metadata":{"id":"bo1g3hUuGr1f"},"execution_count":null,"outputs":[],"id":"bo1g3hUuGr1f"},{"cell_type":"markdown","source":["Defining NLP pipeline and grid search parameters, fitting model:"],"metadata":{"id":"uM3ZVoo8Gr1f"},"id":"uM3ZVoo8Gr1f"},{"cell_type":"code","source":["pipeline = Pipeline([\n","    ('vectorizer', CountVectorizer(analyzer=stemmed_words)),\n","    ('tfidf', TfidfTransformer()),\n","    ('binary_regression', LogisticRegression(solver=\"lbfgs\", max_iter=100000)) #one vs rest\n","])\n","\n","parameters = {'binary_regression__C': [0.01, 0.1, 0.5, 1, 2, 5, 10, 100, 1000], # other parameters? inverse of l2 regularization, larger C more prone to overfit\n","              'vectorizer__stop_words': ['english'],\n","              'vectorizer__max_features': [100]}\n","\n","pipeline_gridsearch_cv = GridSearchCV(pipeline, parameters, cv=5, verbose=1)\n","pipeline_gridsearch_cv.fit(train_texts, train_labels)\n","best_predictions = pipeline_gridsearch_cv.predict(test_texts)\n","\n","baseline_accuracy = np.mean(best_predictions == test_labels)\n","print(\"TF-IDF Accuracy:\", baseline_accuracy)\n","print(classification_report(test_labels, best_predictions))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667936497534,"user_tz":300,"elapsed":214962,"user":{"displayName":"Justin Xu","userId":"13320263107479278707"}},"outputId":"7f73dce7-6e57-4227-d507-2d6bee53dc9e","id":"0ywHWA5JGr1f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 5 folds for each of 9 candidates, totalling 45 fits\n","TF-IDF Accuracy: 0.6575342465753424\n","              precision    recall  f1-score   support\n","\n","           0       0.66      0.98      0.79       662\n","           1       0.66      0.06      0.11       360\n","\n","    accuracy                           0.66      1022\n","   macro avg       0.66      0.52      0.45      1022\n","weighted avg       0.66      0.66      0.55      1022\n","\n"]}],"id":"0ywHWA5JGr1f"},{"cell_type":"markdown","source":["Printing feature words:"],"metadata":{"id":"nKzS8v-bGr1f"},"id":"nKzS8v-bGr1f"},{"cell_type":"code","source":["with np.printoptions(threshold=np.inf):\n","    print(pipeline_gridsearch_cv.best_estimator_.named_steps['vectorizer'].get_feature_names_out())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667936511642,"user_tz":300,"elapsed":136,"user":{"displayName":"Justin Xu","userId":"13320263107479278707"}},"outputId":"9f52230b-c150-4438-d7fa-8c3f70447483","id":"t3n2kDwlGr1f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['admit' 'after' 'again' 'alreadi' 'also' 'an' 'and' 'are' 'as' 'at' 'be'\n"," 'becam' 'becaus' 'been' 'befor' 'blood' 'bodi' 'breath' 'brought' 'but'\n"," 'by' 'client' 'day' 'death' 'deceas' 'did' 'die' 'difficulti' 'doctor'\n"," 'due' 'even' 'fever' 'for' 'from' 'given' 'got' 'had' 'has' 'have' 'he'\n"," 'heart' 'her' 'him' 'his' 'home' 'hospit' 'hospital2' 'in' 'interview'\n"," 'is' 'it' 'last' 'medicin' 'month' 'my' 'no' 'not' 'of' 'on' 'one' 'onli'\n"," 'oper' 'out' 'pain' 'patient' 'place' 'problem' 'said' 'servic' 'she'\n"," 'sinc' 'so' 'start' 'suffer' 'take' 'taken' 'that' 'the' 'then' 'there'\n"," 'they' 'this' 'time' 'to' 'told' 'took' 'treatment' 'up' 'us' 'use'\n"," 'veri' 'was' 'we' 'went' 'were' 'when' 'which' 'with' 'would' 'year']\n"]}],"id":"t3n2kDwlGr1f"},{"cell_type":"markdown","source":["### Classification of Gender - 2 Classes\n"],"metadata":{"id":"bIF_EayRGx60"},"id":"bIF_EayRGx60"},{"cell_type":"markdown","source":["There are a few records that have NaN as gender. Given the low number of records (10), they will be removed before model fitting."],"metadata":{"id":"91ln71UkuAQm"},"id":"91ln71UkuAQm"},{"cell_type":"code","source":["print(f\"Possible genders: {list(df['g5_02'].unique())}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kOj8pM4guMSs","executionInfo":{"status":"ok","timestamp":1667936530312,"user_tz":300,"elapsed":158,"user":{"displayName":"Justin Xu","userId":"13320263107479278707"}},"outputId":"f3f01e03-a0cb-4de4-a3f2-898e26d5f219"},"id":"kOj8pM4guMSs","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Possible genders: ['Male', 'Female', nan]\n"]}]},{"cell_type":"code","source":["df[~df['g5_02'].isin(['Male', 'Female'])]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":751},"id":"D9Fq57ByrU5r","executionInfo":{"status":"ok","timestamp":1667936530521,"user_tz":300,"elapsed":213,"user":{"displayName":"Justin Xu","userId":"13320263107479278707"}},"outputId":"236b22ab-ca85-44c5-8f8f-534effa25baa"},"id":"D9Fq57ByrU5r","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      newid    site  sex  age_years  age_months  age_days  \\\n","32       42     Dar  NaN       33.0         NaN       NaN   \n","68       86     Dar  NaN       44.0         NaN       NaN   \n","288     338     Dar  NaN       74.0         NaN       NaN   \n","1864   2160     Dar  NaN       -1.0         NaN       NaN   \n","2291   2662     Dar  NaN       65.0         NaN       NaN   \n","2306   2678  Mexico  NaN       40.0         NaN       NaN   \n","2490   2891     Dar  NaN       48.0         NaN       NaN   \n","4425   5105     Dar  NaN       -1.0         NaN       NaN   \n","4517   5215     Dar  NaN       -1.0         NaN       NaN   \n","6257   7213     Dar  NaN       27.0         NaN       NaN   \n","\n","                            gs_text34  \\\n","32                          Cirrhosis   \n","68                               AIDS   \n","288                            Stroke   \n","1864  Other Non-communicable Diseases   \n","2291                         Diabetes   \n","2306                            Fires   \n","2490                        Cirrhosis   \n","4425                               TB   \n","4517                               TB   \n","6257    Other Cardiovascular Diseases   \n","\n","                                          open_response  g1_01d      g1_01m  \\\n","32                        deceded died of liver problem    -1.0  Don't Know   \n","68    the patricipant think that relative death caus...    -1.0  Don't Know   \n","288   the participant has nothing to add_x000D__x000...    -1.0  Don't Know   \n","1864  the participant satisfied for the medical serv...    -1.0  Don't Know   \n","2291                 the client thanked for the service    -1.0  Don't Know   \n","2306  to take into account that the disease that my ...     NaN         NaN   \n","2490           the client was suffering from eye cancer    -1.0  Don't Know   \n","4425                     client had no additional point    -1.0  Don't Know   \n","4517  the participant perceives that the deceased wa...    -1.0  Don't Know   \n","6257                 the participant had nothing to add    -1.0  Don't Know   \n","\n","      ...  g5_01y g5_02  g5_03d     g5_03m  g5_03y  g5_04a  g5_04b  g5_04c  \\\n","32    ...  1975.0   NaN    22.0  September  2008.0    33.0     NaN     NaN   \n","68    ...  1964.0   NaN    26.0   December  2008.0    44.0     NaN     NaN   \n","288   ...  1935.0   NaN    22.0      April  2009.0    74.0     NaN     NaN   \n","1864  ...    -1.0   NaN    -1.0       June  2009.0    -1.0     NaN     NaN   \n","2291  ...  1943.0   NaN     4.0    October  2008.0    65.0     NaN     NaN   \n","2306  ...     NaN   NaN    28.0      March  2009.0    40.0     NaN     NaN   \n","2490  ...  1961.0   NaN    20.0      March  2009.0    48.0     NaN     NaN   \n","4425  ...    -1.0   NaN    20.0   December  2008.0    -1.0     NaN     NaN   \n","4517  ...    -1.0   NaN    20.0   November  2008.0    -1.0     NaN     NaN   \n","6257  ...  1981.0   NaN     3.0       June  2009.0    27.0     NaN     NaN   \n","\n","              g5_06a g5_06b  \n","32    Primary School      0  \n","68    Primary School      0  \n","288   Primary School      0  \n","1864    No Schooling      0  \n","2291  Primary School      0  \n","2306  Primary School      6  \n","2490  Primary School      0  \n","4425    No Schooling      0  \n","4517    No Schooling      0  \n","6257  Primary School      0  \n","\n","[10 rows x 53 columns]"],"text/html":["\n","  <div id=\"df-f91cb234-6c08-468e-8d70-cce6b82be7d7\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>newid</th>\n","      <th>site</th>\n","      <th>sex</th>\n","      <th>age_years</th>\n","      <th>age_months</th>\n","      <th>age_days</th>\n","      <th>gs_text34</th>\n","      <th>open_response</th>\n","      <th>g1_01d</th>\n","      <th>g1_01m</th>\n","      <th>...</th>\n","      <th>g5_01y</th>\n","      <th>g5_02</th>\n","      <th>g5_03d</th>\n","      <th>g5_03m</th>\n","      <th>g5_03y</th>\n","      <th>g5_04a</th>\n","      <th>g5_04b</th>\n","      <th>g5_04c</th>\n","      <th>g5_06a</th>\n","      <th>g5_06b</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>32</th>\n","      <td>42</td>\n","      <td>Dar</td>\n","      <td>NaN</td>\n","      <td>33.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Cirrhosis</td>\n","      <td>deceded died of liver problem</td>\n","      <td>-1.0</td>\n","      <td>Don't Know</td>\n","      <td>...</td>\n","      <td>1975.0</td>\n","      <td>NaN</td>\n","      <td>22.0</td>\n","      <td>September</td>\n","      <td>2008.0</td>\n","      <td>33.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Primary School</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>68</th>\n","      <td>86</td>\n","      <td>Dar</td>\n","      <td>NaN</td>\n","      <td>44.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>AIDS</td>\n","      <td>the patricipant think that relative death caus...</td>\n","      <td>-1.0</td>\n","      <td>Don't Know</td>\n","      <td>...</td>\n","      <td>1964.0</td>\n","      <td>NaN</td>\n","      <td>26.0</td>\n","      <td>December</td>\n","      <td>2008.0</td>\n","      <td>44.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Primary School</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>288</th>\n","      <td>338</td>\n","      <td>Dar</td>\n","      <td>NaN</td>\n","      <td>74.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Stroke</td>\n","      <td>the participant has nothing to add_x000D__x000...</td>\n","      <td>-1.0</td>\n","      <td>Don't Know</td>\n","      <td>...</td>\n","      <td>1935.0</td>\n","      <td>NaN</td>\n","      <td>22.0</td>\n","      <td>April</td>\n","      <td>2009.0</td>\n","      <td>74.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Primary School</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1864</th>\n","      <td>2160</td>\n","      <td>Dar</td>\n","      <td>NaN</td>\n","      <td>-1.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Other Non-communicable Diseases</td>\n","      <td>the participant satisfied for the medical serv...</td>\n","      <td>-1.0</td>\n","      <td>Don't Know</td>\n","      <td>...</td>\n","      <td>-1.0</td>\n","      <td>NaN</td>\n","      <td>-1.0</td>\n","      <td>June</td>\n","      <td>2009.0</td>\n","      <td>-1.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>No Schooling</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2291</th>\n","      <td>2662</td>\n","      <td>Dar</td>\n","      <td>NaN</td>\n","      <td>65.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Diabetes</td>\n","      <td>the client thanked for the service</td>\n","      <td>-1.0</td>\n","      <td>Don't Know</td>\n","      <td>...</td>\n","      <td>1943.0</td>\n","      <td>NaN</td>\n","      <td>4.0</td>\n","      <td>October</td>\n","      <td>2008.0</td>\n","      <td>65.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Primary School</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2306</th>\n","      <td>2678</td>\n","      <td>Mexico</td>\n","      <td>NaN</td>\n","      <td>40.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Fires</td>\n","      <td>to take into account that the disease that my ...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>28.0</td>\n","      <td>March</td>\n","      <td>2009.0</td>\n","      <td>40.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Primary School</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>2490</th>\n","      <td>2891</td>\n","      <td>Dar</td>\n","      <td>NaN</td>\n","      <td>48.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Cirrhosis</td>\n","      <td>the client was suffering from eye cancer</td>\n","      <td>-1.0</td>\n","      <td>Don't Know</td>\n","      <td>...</td>\n","      <td>1961.0</td>\n","      <td>NaN</td>\n","      <td>20.0</td>\n","      <td>March</td>\n","      <td>2009.0</td>\n","      <td>48.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Primary School</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4425</th>\n","      <td>5105</td>\n","      <td>Dar</td>\n","      <td>NaN</td>\n","      <td>-1.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>TB</td>\n","      <td>client had no additional point</td>\n","      <td>-1.0</td>\n","      <td>Don't Know</td>\n","      <td>...</td>\n","      <td>-1.0</td>\n","      <td>NaN</td>\n","      <td>20.0</td>\n","      <td>December</td>\n","      <td>2008.0</td>\n","      <td>-1.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>No Schooling</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4517</th>\n","      <td>5215</td>\n","      <td>Dar</td>\n","      <td>NaN</td>\n","      <td>-1.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>TB</td>\n","      <td>the participant perceives that the deceased wa...</td>\n","      <td>-1.0</td>\n","      <td>Don't Know</td>\n","      <td>...</td>\n","      <td>-1.0</td>\n","      <td>NaN</td>\n","      <td>20.0</td>\n","      <td>November</td>\n","      <td>2008.0</td>\n","      <td>-1.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>No Schooling</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6257</th>\n","      <td>7213</td>\n","      <td>Dar</td>\n","      <td>NaN</td>\n","      <td>27.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Other Cardiovascular Diseases</td>\n","      <td>the participant had nothing to add</td>\n","      <td>-1.0</td>\n","      <td>Don't Know</td>\n","      <td>...</td>\n","      <td>1981.0</td>\n","      <td>NaN</td>\n","      <td>3.0</td>\n","      <td>June</td>\n","      <td>2009.0</td>\n","      <td>27.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Primary School</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10 rows × 53 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f91cb234-6c08-468e-8d70-cce6b82be7d7')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-f91cb234-6c08-468e-8d70-cce6b82be7d7 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-f91cb234-6c08-468e-8d70-cce6b82be7d7');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["df_filtered = df[df['g5_02'].isin(['Male', 'Female'])]"],"metadata":{"id":"y91iCMZAuYDT"},"id":"y91iCMZAuYDT","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Splitting data and encoding labels:"],"metadata":{"id":"gjT_H23at_by"},"id":"gjT_H23at_by"},{"cell_type":"code","source":["documents = df_filtered['open_response'].tolist()\n","label2id = {\n","    'Male': 0,\n","    'Female': 1\n","}\n","labels = df_filtered['g5_02'].map(label2id).tolist()\n","\n","# create train/test split\n","train_texts, test_texts, train_labels, test_labels = train_test_split(documents, labels, test_size=.15, random_state=8573)\n","\n","# create train/validation split\n","train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2, random_state=3820)"],"metadata":{"id":"vWncZ4zKGx60"},"execution_count":null,"outputs":[],"id":"vWncZ4zKGx60"},{"cell_type":"markdown","source":["Defining analyzer with stemmer:"],"metadata":{"id":"U20LkBz9Gx60"},"id":"U20LkBz9Gx60"},{"cell_type":"code","source":["stemmer = EnglishStemmer()\n","analyzer = CountVectorizer().build_analyzer()\n","def stemmed_words(doc):\n","    return (stemmer.stem(w) for w in analyzer(doc))"],"metadata":{"id":"Dhekwl8zGx60"},"execution_count":null,"outputs":[],"id":"Dhekwl8zGx60"},{"cell_type":"markdown","source":["Defining NLP pipeline and grid search parameters, fitting model:"],"metadata":{"id":"nHn5M_GnGx60"},"id":"nHn5M_GnGx60"},{"cell_type":"code","source":["pipeline = Pipeline([\n","    ('vectorizer', CountVectorizer(analyzer=stemmed_words)),\n","    ('tfidf', TfidfTransformer()),\n","    ('binary_regression', LogisticRegression(solver=\"lbfgs\", max_iter=100000)) #one vs rest\n","])\n","\n","parameters = {'binary_regression__C': [0.01, 0.1, 0.5, 1, 2, 5, 10, 100, 1000], # other parameters? inverse of l2 regularization, larger C more prone to overfit\n","              'vectorizer__stop_words': ['english'],\n","              'vectorizer__max_features': [100]}\n","\n","pipeline_gridsearch_cv = GridSearchCV(pipeline, parameters, cv=5, verbose=1)\n","pipeline_gridsearch_cv.fit(train_texts, train_labels)\n","best_predictions = pipeline_gridsearch_cv.predict(test_texts)\n","\n","baseline_accuracy = np.mean(best_predictions == test_labels)\n","print(\"TF-IDF Accuracy:\", baseline_accuracy)\n","print(classification_report(test_labels, best_predictions))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667936730946,"user_tz":300,"elapsed":200427,"user":{"displayName":"Justin Xu","userId":"13320263107479278707"}},"outputId":"8c0c11a2-efff-4b92-cb86-8814c807984c","id":"CkHRGu_xGx60"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 5 folds for each of 9 candidates, totalling 45 fits\n","TF-IDF Accuracy: 0.7705882352941177\n","              precision    recall  f1-score   support\n","\n","           0       0.78      0.79      0.79       549\n","           1       0.76      0.74      0.75       471\n","\n","    accuracy                           0.77      1020\n","   macro avg       0.77      0.77      0.77      1020\n","weighted avg       0.77      0.77      0.77      1020\n","\n"]}],"id":"CkHRGu_xGx60"},{"cell_type":"markdown","source":["Printing feature words:"],"metadata":{"id":"SdY06yxSGx60"},"id":"SdY06yxSGx60"},{"cell_type":"code","source":["with np.printoptions(threshold=np.inf):\n","    print(pipeline_gridsearch_cv.best_estimator_.named_steps['vectorizer'].get_feature_names_out())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667936738941,"user_tz":300,"elapsed":140,"user":{"displayName":"Justin Xu","userId":"13320263107479278707"}},"outputId":"b6f12350-d3f3-4bba-c81d-9fa4eb4b4d4d","id":"GyrZwGG3Gx60"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['admit' 'after' 'again' 'alreadi' 'also' 'an' 'and' 'are' 'as' 'at' 'be'\n"," 'becam' 'becaus' 'been' 'befor' 'blood' 'bodi' 'breath' 'brought' 'but'\n"," 'by' 'client' 'day' 'death' 'deceas' 'did' 'die' 'difficulti' 'doctor'\n"," 'due' 'even' 'fever' 'for' 'from' 'given' 'go' 'got' 'had' 'has' 'have'\n"," 'he' 'heart' 'her' 'him' 'his' 'home' 'hospit' 'hospital2' 'in'\n"," 'interview' 'is' 'it' 'last' 'medicin' 'month' 'my' 'no' 'not' 'of' 'on'\n"," 'one' 'onli' 'oper' 'out' 'pain' 'patient' 'place' 'problem' 'said'\n"," 'servic' 'she' 'sinc' 'so' 'start' 'suffer' 'take' 'taken' 'that' 'the'\n"," 'then' 'there' 'they' 'time' 'to' 'told' 'took' 'treatment' 'up' 'us'\n"," 'use' 'veri' 'was' 'we' 'went' 'were' 'when' 'which' 'with' 'would'\n"," 'year']\n"]}],"id":"GyrZwGG3Gx60"}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[{"file_id":"1kMdKEhj8EOvYzhJk1oPzGgpSJch_FrUk","timestamp":1665627396496},{"file_id":"https://github.com/kind-lab/verbal-autopsy-sdoh/blob/main/02-train-model.ipynb","timestamp":1665598085896}],"collapsed_sections":["ghTeDZAOUFS_","WwJ9Z_UU3mfo","E0ft5NBLUVoZ","XhVTemI8VZuG","Ulrngldt7UPQ"]},"widgets":{"application/vnd.jupyter.widget-state+json":{"400b17fecd5946afb64e94cb38af23fa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6a50631f89cd47f7bf966bfd440caa83","IPY_MODEL_155efd5cc36d4c51943618c7fd8442b9","IPY_MODEL_c6b26a52f59047bea3b89d8a4c5a4001"],"layout":"IPY_MODEL_65c14868d1594576a261b78323f67026"}},"6a50631f89cd47f7bf966bfd440caa83":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ec6a3e87e2243529a5c9ca5b5342a8d","placeholder":"​","style":"IPY_MODEL_2627ffea34f0414cac27fc4a90d60546","value":"Downloading: 100%"}},"155efd5cc36d4c51943618c7fd8442b9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d961bb1c79024fc39be7c1bfead655d3","max":440473133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_743e5c3bdfcb42f0add66dcd126d13b8","value":440473133}},"c6b26a52f59047bea3b89d8a4c5a4001":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_622ca94b471345aeb383a31c2036c743","placeholder":"​","style":"IPY_MODEL_aa3c9a9e05a44fd497be86af6e799d62","value":" 440M/440M [00:16&lt;00:00, 52.6MB/s]"}},"65c14868d1594576a261b78323f67026":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ec6a3e87e2243529a5c9ca5b5342a8d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2627ffea34f0414cac27fc4a90d60546":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d961bb1c79024fc39be7c1bfead655d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"743e5c3bdfcb42f0add66dcd126d13b8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"622ca94b471345aeb383a31c2036c743":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa3c9a9e05a44fd497be86af6e799d62":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9b02249a8d6744108e53ddaf24a080b2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d6f8d79db754427d901dfe8eebc1d2b8","IPY_MODEL_9c3fff5a29d34c8fa1a9d54932c82653","IPY_MODEL_c8add35dab854c69984082a867006498"],"layout":"IPY_MODEL_7a0e2b87915a46948e8ca794ee108ca0"}},"d6f8d79db754427d901dfe8eebc1d2b8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3114195fbce46ae94de3de657946dc0","placeholder":"​","style":"IPY_MODEL_78cd482726c64f6da2587cb32927d3cb","value":"Downloading: 100%"}},"9c3fff5a29d34c8fa1a9d54932c82653":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3488165062c243a2bc7865d5600dba93","max":501200538,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fdac075851b040b2a3bdd3e59ee0204a","value":501200538}},"c8add35dab854c69984082a867006498":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_721d5f3cfbc044c2a7a356bc8279d814","placeholder":"​","style":"IPY_MODEL_3541d8c13d74489c8629668ef2ecd77a","value":" 501M/501M [00:14&lt;00:00, 40.4MB/s]"}},"7a0e2b87915a46948e8ca794ee108ca0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3114195fbce46ae94de3de657946dc0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78cd482726c64f6da2587cb32927d3cb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3488165062c243a2bc7865d5600dba93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdac075851b040b2a3bdd3e59ee0204a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"721d5f3cfbc044c2a7a356bc8279d814":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3541d8c13d74489c8629668ef2ecd77a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a31db033354245bcba0937764108c813":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5645c4cc81b4411eb8312d05fa74888a","IPY_MODEL_57899bddd571446fbd4279815d651021","IPY_MODEL_81bf2c4f15b2455bbb80e5cac80d774e"],"layout":"IPY_MODEL_1fd84fe1b5ee40b6b18f0aa12e601c03"}},"5645c4cc81b4411eb8312d05fa74888a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_06105feeee1c481cab396ab1a73d9276","placeholder":"​","style":"IPY_MODEL_97360fd5c3854398a9ae8109525e932d","value":"Downloading: 100%"}},"57899bddd571446fbd4279815d651021":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5ad953ba313413296aa1fdb5c9a5b8b","max":481,"min":0,"orientation":"horizontal","style":"IPY_MODEL_96a2fce3ffca4326a12ffbc068ae5a7a","value":481}},"81bf2c4f15b2455bbb80e5cac80d774e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b95b81eb19c44ce98cf2a29c7d7c8373","placeholder":"​","style":"IPY_MODEL_a397cf9f2a5c4ecbbd9d84b4559fb84b","value":" 481/481 [00:00&lt;00:00, 13.9kB/s]"}},"1fd84fe1b5ee40b6b18f0aa12e601c03":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06105feeee1c481cab396ab1a73d9276":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97360fd5c3854398a9ae8109525e932d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5ad953ba313413296aa1fdb5c9a5b8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96a2fce3ffca4326a12ffbc068ae5a7a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b95b81eb19c44ce98cf2a29c7d7c8373":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a397cf9f2a5c4ecbbd9d84b4559fb84b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"38d6ed2bf6c44d81893f32ff0a448b38":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2f62e19f754540bea612fd8bea0d8531","IPY_MODEL_4683ced2246d4c16ae567430d054afff","IPY_MODEL_41c7dc16eba8477ebd149f2e60cf6d3d"],"layout":"IPY_MODEL_f9594fa587b34a7f831929dce288b036"}},"2f62e19f754540bea612fd8bea0d8531":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b3841c7b4fca432d94b574c51493ddcf","placeholder":"​","style":"IPY_MODEL_04d81f735d06437183969995c7ecbf64","value":"Downloading: 100%"}},"4683ced2246d4c16ae567430d054afff":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4e96a99b8b74cb6800a6dd136d68bad","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_72ca6294e99a43c4a43b3289302c4367","value":898823}},"41c7dc16eba8477ebd149f2e60cf6d3d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8797c015403f4c5d9db61cecf5aa68be","placeholder":"​","style":"IPY_MODEL_2a1e72a9c6a94e4885d7fb17f59f42fe","value":" 899k/899k [00:00&lt;00:00, 954kB/s]"}},"f9594fa587b34a7f831929dce288b036":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3841c7b4fca432d94b574c51493ddcf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"04d81f735d06437183969995c7ecbf64":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e4e96a99b8b74cb6800a6dd136d68bad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72ca6294e99a43c4a43b3289302c4367":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8797c015403f4c5d9db61cecf5aa68be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a1e72a9c6a94e4885d7fb17f59f42fe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b46abdd7772544848c5a49f4ef67188b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_757af5f6557c496696fdab4b1a5cca09","IPY_MODEL_5f109f7463564ac9ba2be62d51550f32","IPY_MODEL_8aed795ce59f4cfeafb719ecf723e695"],"layout":"IPY_MODEL_c563997b2e2645759ee6cc6fa79df413"}},"757af5f6557c496696fdab4b1a5cca09":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_84adf48dc3624a03815fa0073ee871b9","placeholder":"​","style":"IPY_MODEL_f8b382622a084f39b37aa0e040306b04","value":"Downloading: 100%"}},"5f109f7463564ac9ba2be62d51550f32":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef0a0170ab884b37a1d860e99c8843ec","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_096b41d856844cffa6d2500588e9b059","value":456318}},"8aed795ce59f4cfeafb719ecf723e695":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_35777e7d0cc34d31ad781ead8ca67c4e","placeholder":"​","style":"IPY_MODEL_898845e7c69f4c74bc6e95a6904212d4","value":" 456k/456k [00:00&lt;00:00, 730kB/s]"}},"c563997b2e2645759ee6cc6fa79df413":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84adf48dc3624a03815fa0073ee871b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8b382622a084f39b37aa0e040306b04":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ef0a0170ab884b37a1d860e99c8843ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"096b41d856844cffa6d2500588e9b059":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"35777e7d0cc34d31ad781ead8ca67c4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"898845e7c69f4c74bc6e95a6904212d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"67e7166b71ae4c8dafd986ef36c87470":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b90eb48fc1c4400da43e24d105f0e505","IPY_MODEL_017dbb114995445abe466165f1b4ddfa","IPY_MODEL_6e068e4846c7458ead18db9a983d5730"],"layout":"IPY_MODEL_99eece6541854e16a2a696c293509713"}},"b90eb48fc1c4400da43e24d105f0e505":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aba950e16f704dac9fc1d20249236aae","placeholder":"​","style":"IPY_MODEL_cae3b6d947fa40458df269c01468b675","value":"Downloading: 100%"}},"017dbb114995445abe466165f1b4ddfa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_097431f0085f4204bdc49c843f7df820","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c41a0f0455a447368d3e2059a190c4b5","value":1355863}},"6e068e4846c7458ead18db9a983d5730":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f70419e6f4647f186d209f6fb2e8f3b","placeholder":"​","style":"IPY_MODEL_20157452c28a4056ac3fed3c8694826d","value":" 1.36M/1.36M [00:00&lt;00:00, 1.48MB/s]"}},"99eece6541854e16a2a696c293509713":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aba950e16f704dac9fc1d20249236aae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cae3b6d947fa40458df269c01468b675":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"097431f0085f4204bdc49c843f7df820":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c41a0f0455a447368d3e2059a190c4b5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2f70419e6f4647f186d209f6fb2e8f3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20157452c28a4056ac3fed3c8694826d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"427082df5e8d45778b9597efdc7deb6e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dfe09275fe9248ebb3ad7f783b97c456","IPY_MODEL_24ad4c401953458ea278d18140f75c49","IPY_MODEL_10c686f5101f48489e4ef0d6c86adf5f"],"layout":"IPY_MODEL_a5ee06de7a4b464aaa9ade8646c6c6f2"}},"dfe09275fe9248ebb3ad7f783b97c456":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0971f51261c447ec8a8ec67a1afe6639","placeholder":"​","style":"IPY_MODEL_64e29477e69241e59eb6dac18ffb5a30","value":"Downloading: 100%"}},"24ad4c401953458ea278d18140f75c49":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_00528b45220b41dc89416a5097fc24e2","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_905ba61e0c0e4a428b1b30ab953ff10c","value":28}},"10c686f5101f48489e4ef0d6c86adf5f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_79713d149d654e5b8cc17684068e5dad","placeholder":"​","style":"IPY_MODEL_771c0d286607441897228d181c91aab4","value":" 28.0/28.0 [00:00&lt;00:00, 277B/s]"}},"a5ee06de7a4b464aaa9ade8646c6c6f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0971f51261c447ec8a8ec67a1afe6639":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64e29477e69241e59eb6dac18ffb5a30":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"00528b45220b41dc89416a5097fc24e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"905ba61e0c0e4a428b1b30ab953ff10c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"79713d149d654e5b8cc17684068e5dad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"771c0d286607441897228d181c91aab4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ce7878827bdc4ac48435ff447cf5e881":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_428223f5f04c452ca718377515cfa730","IPY_MODEL_6ac2abed8d75412f88731058cbda76ba","IPY_MODEL_578d5f679f89461a8c007623464548f9"],"layout":"IPY_MODEL_dcccea5ec24346fda61c1bc6714937fa"}},"428223f5f04c452ca718377515cfa730":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_623be4e7ec884d669f0b8e304d4d40a0","placeholder":"​","style":"IPY_MODEL_ce631303afd64aee890e947810ddbe4a","value":"Downloading: 100%"}},"6ac2abed8d75412f88731058cbda76ba":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c27b1bc00ec848edbcc30548146d13b3","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d076abb487694a3ab2127bbefd20b992","value":570}},"578d5f679f89461a8c007623464548f9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d88883a0a954593b4dbacfb8081ea2a","placeholder":"​","style":"IPY_MODEL_7c212c9765344da5b1c63838dc17e2d3","value":" 570/570 [00:00&lt;00:00, 6.57kB/s]"}},"dcccea5ec24346fda61c1bc6714937fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"623be4e7ec884d669f0b8e304d4d40a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce631303afd64aee890e947810ddbe4a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c27b1bc00ec848edbcc30548146d13b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d076abb487694a3ab2127bbefd20b992":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5d88883a0a954593b4dbacfb8081ea2a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c212c9765344da5b1c63838dc17e2d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d16153d4a95140ea8d14cd4026cb6eea":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0b355387f8c0460cac760120460ab39d","IPY_MODEL_f7a5dbcaa48c4ee8afe19e2714817315","IPY_MODEL_b474a4a52a4640e589f59afd87aa7b39"],"layout":"IPY_MODEL_f3ec5561d8da4781bb177b6d2c7891e0"}},"0b355387f8c0460cac760120460ab39d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6dbb317bf9b4102b715785424d661a2","placeholder":"​","style":"IPY_MODEL_ee069f65cc7246eabc3f60e366dd3410","value":"Downloading: 100%"}},"f7a5dbcaa48c4ee8afe19e2714817315":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b3a96b921f994bc5bad74fe151090987","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_515bd1fae02d488da83f032bb3c13b2d","value":231508}},"b474a4a52a4640e589f59afd87aa7b39":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f41a60d40806490fac9d3627a73a9f97","placeholder":"​","style":"IPY_MODEL_1eb7d539fd204a669d4d5a28ac2adc7e","value":" 232k/232k [00:00&lt;00:00, 585kB/s]"}},"f3ec5561d8da4781bb177b6d2c7891e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6dbb317bf9b4102b715785424d661a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee069f65cc7246eabc3f60e366dd3410":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b3a96b921f994bc5bad74fe151090987":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"515bd1fae02d488da83f032bb3c13b2d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f41a60d40806490fac9d3627a73a9f97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1eb7d539fd204a669d4d5a28ac2adc7e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2763ac83339f48dfb3e6cead080845da":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f1f6618249cc43a3b19b1fb8b8754a48","IPY_MODEL_5f6d82b7e87c40cbb9c649d6b23f6f6e","IPY_MODEL_c47fa99539ed449686f1aa0aede3e2bd"],"layout":"IPY_MODEL_8ad95787097e406e939899dc4b0e0a05"}},"f1f6618249cc43a3b19b1fb8b8754a48":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ee4306bb67a4a138665aaf29f0c9434","placeholder":"​","style":"IPY_MODEL_24ffc49127ee42bcab9f757db8ba6565","value":"Downloading: 100%"}},"5f6d82b7e87c40cbb9c649d6b23f6f6e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_661682f02b4d4105a9e16d110dcb832e","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7c135910dad144e18044d26582a52f89","value":466062}},"c47fa99539ed449686f1aa0aede3e2bd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2334e5de866343aba7f2ac0d0b8de5df","placeholder":"​","style":"IPY_MODEL_84b76e1d52bb40f0b2fea289d03e5ab4","value":" 466k/466k [00:00&lt;00:00, 470kB/s]"}},"8ad95787097e406e939899dc4b0e0a05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ee4306bb67a4a138665aaf29f0c9434":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24ffc49127ee42bcab9f757db8ba6565":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"661682f02b4d4105a9e16d110dcb832e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c135910dad144e18044d26582a52f89":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2334e5de866343aba7f2ac0d0b8de5df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84b76e1d52bb40f0b2fea289d03e5ab4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"gpuClass":"standard","accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}